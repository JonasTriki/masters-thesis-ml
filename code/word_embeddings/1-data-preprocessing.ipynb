{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/triki/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/triki/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pickle\n",
    "import numpy as np\n",
    "rng_seed = 399\n",
    "np.random.seed(rng_seed)\n",
    "from typing import Union\n",
    "from os.path import join as join_path\n",
    "from utils import filter_word, clean_sents\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and preprocess data\n",
    "We download the raw enwik8 data: http://mattmahoney.net/dc/textdata.html. Furthermore, we apply the wikifil.pl Perl script as specified in [Appendix A](http://mattmahoney.net/dc/textdata.html#appendixa) to parse the raw Wikipedia dump to clean text. We have made a minor justification to the last bit of the Perl script to further exclude the period \".\" character such that we can split the text into sentences later (from `tr/a-z/ /cs;` to `tr/a-z\\./ /cs;`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessing = False\n",
    "if data_preprocessing:\n",
    "    \n",
    "    # Download the raw data\n",
    "    !wget -c http://mattmahoney.net/dc/enwik8.zip -P data\n",
    "    !unzip data/enwik8.zip -d data\n",
    "\n",
    "    # A raw Wikipedia dump contains a lot of HTML / XML data.\n",
    "    # We pre-process it with the wikifil.pl script\n",
    "    # (originally developed by Matt Mahoney, and can be found on his website).\n",
    "    !perl data/wikifil.pl data/enwik8 > data/text8\n",
    "\n",
    "    # Sanity checking first words of new file\n",
    "    !head -c 2000 data/text8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "# ----------------\n",
    "data_dir = 'data'\n",
    "text8_data_path = join_path(data_dir, 'text8')\n",
    "text8_data_tokenizer_config_path = join_path(data_dir, 'text8-tokenizer.json')\n",
    "text8_data_sequences_path = join_path(data_dir, 'text8-seqs.p')\n",
    "max_vocab_size = 1000\n",
    "# ----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean dataset\n",
    "To clean the dataset, we first use `sent_tokenize` from [NLTK](https://www.nltk.org/api/nltk.tokenize.html) to split the dataset into sentences. It is more convenient to use sentences instead of a big text, due to computational restrictions. To further clean the sentences, we convert the words to lowercase, apply lemmatization, filter out stopwords and remove words that are one character long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31621ce9f28c440bb72b17d4d21e4865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=504749.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['anarchism originated term abuse first used early working class radical including digger english revolution sans culotte french revolution',\n",
       " 'whilst term still used pejorative way describe act used violent mean destroy organization society also taken positive label self defined anarchist',\n",
       " 'word anarchism derived greek without archons ruler chief king',\n",
       " 'anarchism political philosophy belief ruler unnecessary abolished although differing interpretation mean',\n",
       " 'anarchism also refers related social movement advocate elimination authoritarian institution particularly state',\n",
       " 'word anarchy anarchist use imply chaos nihilism anomie rather harmonious anti authoritarian society',\n",
       " 'place regarded authoritarian political structure coercive economic institution anarchist advocate social relation based upon voluntary association autonomous individual mutual aid self governance',\n",
       " 'anarchism easily defined anarchist also offer positive vision believe truly free society',\n",
       " 'however idea anarchist society might work vary considerably especially respect economics also disagreement free society might brought',\n",
       " 'origin predecessor kropotkin others argue recorded history human society organized anarchist principle']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(text8_data_path, 'r') as file:\n",
    "    text8_content = file.read()\n",
    "text8_sents = sent_tokenize(text8_content)\n",
    "\n",
    "# Remove stop words and lemmatize\n",
    "text8_sents = clean_sents(text8_sents, verbose=True)\n",
    "text8_sents[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use Tensorflows Tokenizer class to tokenize the sentences. The tokenization process starts with calling `fit_on_texts` which creates a vocabulary out of the top-N most common words. In our case, we are interested in the top 1000 words and fit the Tokenizer accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vocabulary...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f19e7ce5dd4a91a65f1c69fcf32564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=504749.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Create vocabulary from texts\n",
    "fit_tokenizer = False\n",
    "if fit_tokenizer:\n",
    "    print('Creating vocabulary...')\n",
    "    \n",
    "    # Here we set the filters to empty string and lower case to false\n",
    "    # because we have already performed the nessecary preprocessing steps.\n",
    "    tokenizer = Tokenizer(max_vocab_size, filters='', lower=False)\n",
    "    tokenizer.fit_on_texts(tqdm(text8_sents, unit='text'))\n",
    "\n",
    "    # Save to file\n",
    "    with open(text8_data_tokenizer_config_path, 'w') as file:\n",
    "            file.write(tokenizer.to_json())\n",
    "    print('Done!')\n",
    "else:\n",
    "    print('Reading vocabulary...')\n",
    "    \n",
    "    # Read tokenizer from file\n",
    "    with open(text8_data_tokenizer_config_path, 'r') as file:\n",
    "        tokenizer = tokenizer_from_json(file.read())\n",
    "    print('Done!')\n",
    "\n",
    "vocab_size = np.minimum(max_vocab_size, len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a Tokenizer that has created a vocabulary for our sentences. We now convert them into sequences of numbers corresponding to how frequent then occur in the sentences. We also write the sequences to disk for easier use later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7712f510af4b90b04431953a0637dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=504749.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text8_seqs = tokenizer.texts_to_sequences(tqdm(text8_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(188, 239)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity checks\n",
    "tokenizer.word_index['man'], tokenizer.word_index['woman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write text8 sequences to file\n",
    "with open(text8_data_sequences_path, 'wb') as file:\n",
    "    pickle.dump(text8_seqs, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
