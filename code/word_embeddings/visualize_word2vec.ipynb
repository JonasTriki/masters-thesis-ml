{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "rng_seed = 399\n",
    "np.random.seed(rng_seed)\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from umap import UMAP\n",
    "from configparser import ConfigParser\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import plotly.offline as pyo\n",
    "pyo.init_notebook_mode()\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from importlib import reload\n",
    "import eval_utils\n",
    "import utils\n",
    "reload(eval_utils)\n",
    "reload(utils)\n",
    "\n",
    "from eval_utils import (\n",
    "    get_word_vec,\n",
    "    similar_words,\n",
    "    create_embeddings_of_train_weight_checkpoints,\n",
    "    visualize_embeddings_over_time,\n",
    "    plot_word_relationships_2d,\n",
    "    plot_word_vectors,\n",
    "    evaluate_model_word_analogies\n",
    ")\n",
    "from utils import get_model_checkpoint_filepaths\n",
    "from word2vec import Word2vec, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "output_dir = \"../output/word2vec_training/31-Oct-2020_14-45-28\"\n",
    "checkpoint_filepaths_dict = get_model_checkpoint_filepaths(\n",
    "    output_dir=output_dir,\n",
    "    model_name=\"word2vec\",\n",
    "    dataset_name=\"enwiki\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model training configuration\n",
    "model_training_conf = ConfigParser()\n",
    "model_training_conf.read(checkpoint_filepaths_dict[\"model_training_conf_filepath\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load words and create word to int lookup dict\n",
    "with open(checkpoint_filepaths_dict[\"train_words_filepath\"], \"r\") as file:\n",
    "    words = np.array(file.read().split(\"\\n\"))\n",
    "word_to_int = {word: i for i, word in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vocabulary size, embedding dimension, word to int dictionary and words used in the models\n",
    "vocab_size = model_training_conf[\"MODELCONFIG\"].getint(\"vocab_size\")\n",
    "embedding_dim = model_training_conf[\"MODELCONFIG\"].getint(\"embedding_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get target embedding weights of last model\n",
    "last_embedding_weights_filepath = checkpoint_filepaths_dict[\"intermediate_embedding_weight_filepaths\"][-1]\n",
    "last_embedding_weights = np.load(last_embedding_weights_filepath, mmap_mode=\"r\").astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize training over the course of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to computational limitations, we only visualize the top 1000 most common words.\n",
    "vis_embeddings_vocab_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings of word embeddings from all train checkpoints\n",
    "umap_embeddings_over_time, cluster_labels_over_time = create_embeddings_of_train_weight_checkpoints(\n",
    "    model_weights_filepaths=checkpoint_filepaths_dict[\"intermediate_embedding_weight_filepaths\"],\n",
    "    vocab_size=vis_embeddings_vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    clusterer=KMeans(n_clusters=10, random_state=rng_seed),\n",
    "    transformer=UMAP(n_components=3, random_state=rng_seed)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training\n",
    "visualize_embeddings_over_time(\n",
    "    transformed_word_embeddings=umap_embeddings_over_time,\n",
    "    cluster_labels=cluster_labels_over_time,\n",
    "    vocab_size=vis_embeddings_vocab_size,\n",
    "    words=words\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find closest word to\n",
    "similar_words(\n",
    "    positive_words=[\"man\"],\n",
    "    weights=last_embedding_weights,\n",
    "    word_to_int=word_to_int,\n",
    "    words=words,\n",
    "    top_n=10,\n",
    "    vocab_size=100000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test similarities\n",
    "similar_words(\n",
    "    positive_words=[\"woman\", \"king\"],\n",
    "    negative_words=[\"man\"],\n",
    "    weights=last_embedding_weights,\n",
    "    word_to_int=word_to_int,\n",
    "    words=words,\n",
    "    top_n=10,\n",
    "    vocab_size=100000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot word relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2D PCA embeddings of last model\n",
    "embedding_weights_2d_pca = PCA(n_components=2, random_state=rng_seed).fit_transform(last_embedding_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [\n",
    "    ('man', 'woman'),\n",
    "    ('king', 'queen')\n",
    "]\n",
    "plot_word_relationships_2d(\n",
    "    pairs,\n",
    "    embedding_weights_2d_pca,\n",
    "    word_to_int,\n",
    "    x_label=\"PC 1\",\n",
    "    y_label=\"PC 2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2D UMAP embeddings of last model\n",
    "embedding_weights_2d_umap = UMAP(n_components=2, random_state=rng_seed).fit_transform(last_embedding_weights[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot words one through nine to check for cirular shape\n",
    "zero_to_nine = [\n",
    "    'zero',\n",
    "    'one',\n",
    "    'two',\n",
    "    'three',\n",
    "    'four',\n",
    "    'five',\n",
    "    'six',\n",
    "    'seven',\n",
    "    'eight',\n",
    "    'nine'\n",
    "]\n",
    "plot_word_vectors(\n",
    "    zero_to_nine,\n",
    "    embedding_weights_2d_umap,\n",
    "    word_to_int,\n",
    "    x_label=\"UMAP 1\",\n",
    "    y_label=\"UMAP 2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Country and Capital PCA plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word vectors of countried and capitals\n",
    "countries_to_capitals = [\n",
    "    (\"china\", \"beijing\"),\n",
    "    (\"russia\", \"moscow\"),\n",
    "    (\"japan\", \"tokyo\"),\n",
    "    (\"turkey\", \"ankara\"),\n",
    "    (\"poland\", \"warsaw\"),\n",
    "    (\"germany\", \"berlin\"),\n",
    "    (\"france\", \"paris\"),\n",
    "    (\"italy\", \"rome\"),\n",
    "    (\"greece\", \"athens\"),\n",
    "    (\"spain\", \"madrid\"),\n",
    "    (\"portugal\", \"lisbon\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "ax = plt.axes()\n",
    "for country_word, capital_word in countries_to_capitals:\n",
    "    country_vec = embedding_weights_2d_pca[word_to_int[country_word]]\n",
    "    capital_vec = embedding_weights_2d_pca[word_to_int[capital_word]]\n",
    "    \n",
    "    plt.scatter(country_vec[0], country_vec[1], marker=\"x\")\n",
    "    plt.text(country_vec[0] + 0.003, country_vec[1] + 0.003, country_word, fontsize=12)\n",
    "    \n",
    "    plt.scatter(capital_vec[0], capital_vec[1], marker=\"x\")\n",
    "    plt.text(capital_vec[0] + 0.003, capital_vec[1] + 0.003, capital_word, fontsize=12)\n",
    "    \n",
    "    # Draw arrow\n",
    "    ax.arrow(\n",
    "        country_vec[0],\n",
    "        country_vec[1],\n",
    "        capital_vec[0] - country_vec[0],\n",
    "        capital_vec[1] - country_vec[1],\n",
    "        head_width=0.003,\n",
    "        length_includes_head=True,\n",
    "        ls=\"--\",\n",
    "        color=\"#ddd\"\n",
    "    )\n",
    "plt.title(\"Country and Capital Vectors Projected by PCA\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Cluster alle land i feks 5. cluster => kontinent?\n",
    "# - land\n",
    "# - mat\n",
    "# - land\n",
    "# - språk\n",
    "# - yrker\n",
    "# - sport\n",
    "# - programmeringsspråk (vs. språk)\n",
    "# - musikksjangre"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
