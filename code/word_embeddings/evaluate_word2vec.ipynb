{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "rng_seed = 399\n",
    "np.random.seed(rng_seed)\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from umap import UMAP\n",
    "from configparser import ConfigParser\n",
    "\n",
    "import plotly.offline as pyo\n",
    "pyo.init_notebook_mode()\n",
    "\n",
    "from importlib import reload\n",
    "import eval_utils\n",
    "import train_utils\n",
    "import utils\n",
    "reload(eval_utils)\n",
    "reload(train_utils)\n",
    "reload(utils)\n",
    "\n",
    "from eval_utils import (\n",
    "    get_word_vec,\n",
    "    similar_words,\n",
    "    create_embeddings_of_train_weight_checkpoints,\n",
    "    visualize_embeddings_over_time,\n",
    "    plot_word_relationships_2d,\n",
    "    plot_word_vectors,\n",
    "    evaluate_model_questions_words\n",
    ")\n",
    "from train_utils import get_model_checkpoint_filepaths\n",
    "from utils import load_model\n",
    "from word2vec import Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "checkpoints_dir = \"checkpoints\"\n",
    "checkpoint_filepaths_dict = get_model_checkpoint_filepaths(\n",
    "    checkpoints_dir=checkpoints_dir,\n",
    "    model_name=\"word2vec_sgns\",\n",
    "    dataset_name=\"enwiki\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model training configuration\n",
    "model_training_conf = ConfigParser()\n",
    "model_training_conf.read(checkpoint_filepaths_dict[\"model_training_conf_filepath\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load words and create word to int lookup dict\n",
    "with open(checkpoint_filepaths_dict[\"train_words_filepath\"], \"r\") as file:\n",
    "    words = np.array(file.read().split(\"\\n\"))\n",
    "word_to_int = {word: i for i, word in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load last model\n",
    "# word2vec = load_model(checkpoint_filepaths_dict['model_filepaths'][-1])\n",
    "# word2vec._max_window_size = 5\n",
    "# word2vec.save_model_training_conf(\"checkpoints/word2vec_sgns_enwiki.conf\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vocabulary size, embedding dimension, word to int dictionary and words used in the models\n",
    "vocab_size = model_training_conf[\"MODELCONFIG\"].getint(\"vocab_size\")\n",
    "embedding_dim = model_training_conf[\"MODELCONFIG\"].getint(\"embedding_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get target embedding weights of last model\n",
    "last_embedding_weights = np.load(\n",
    "    checkpoint_filepaths_dict[\"intermediate_embedding_weight_filepaths\"][-1],\n",
    "    mmap_mode=\"r\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize training over the course of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to computational limitations, we only visualize the top 1000 most common words.\n",
    "vis_embeddings_vocab_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings of word embeddings from all train checkpoints\n",
    "umap_embeddings_over_time, cluster_labels_over_time = create_embeddings_of_train_weight_checkpoints(\n",
    "    model_weights_filepaths=checkpoint_filepaths_dict[\"intermediate_embedding_weight_filepaths\"],\n",
    "    vocab_size=vis_embeddings_vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    clusterer=KMeans(n_clusters=10, random_state=rng_seed),\n",
    "    transformer=UMAP(n_components=3, random_state=rng_seed)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training\n",
    "visualize_embeddings_over_time(\n",
    "    transformed_word_embeddings=umap_embeddings_over_time,\n",
    "    cluster_labels=cluster_labels_over_time,\n",
    "    vocab_size=vis_embeddings_vocab_size,\n",
    "    words=words\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find closest word to\n",
    "similar_words(\n",
    "    positive_words=[\"man\"],\n",
    "    weights=last_embedding_weights,\n",
    "    word_to_int=word_to_int,\n",
    "    words=words,\n",
    "    top_n=10,\n",
    "    vocab_size=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test similarities\n",
    "similar_words(\n",
    "    positive_words=[\"france\", \"mogadishu\"],\n",
    "    negative_words=[\"paris\"],\n",
    "    weights=last_embedding_weights,\n",
    "    word_to_int=word_to_int,\n",
    "    words=words,\n",
    "    top_n=10,\n",
    "    vocab_size=100000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot word relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2D PCA embeddings of last model\n",
    "embedding_weights_2d_pca = PCA(n_components=2, random_state=rng_seed).fit_transform(last_embedding_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [\n",
    "    ('man', 'woman'),\n",
    "    ('king', 'queen')\n",
    "]\n",
    "plot_word_relationships_2d(\n",
    "    pairs,\n",
    "    embedding_weights_2d_pca,\n",
    "    word_to_int,\n",
    "    x_label=\"PC 1\",\n",
    "    y_label=\"PC 2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2D UMAP embeddings of last model\n",
    "embedding_weights_2d_umap = UMAP(n_components=2, random_state=rng_seed).fit_transform(last_embedding_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot words one through nine to check for cirular shape\n",
    "zero_to_nine = [\n",
    "    'zero',\n",
    "    'one',\n",
    "    'two',\n",
    "    'three',\n",
    "    'four',\n",
    "    'five',\n",
    "    'six',\n",
    "    'seven',\n",
    "    'eight',\n",
    "    'nine'\n",
    "]\n",
    "plot_word_vectors(\n",
    "    zero_to_nine,\n",
    "    embedding_weights_2d_umap,\n",
    "    word_to_int,\n",
    "    x_label=\"UMAP 1\",\n",
    "    y_label=\"UMAP 2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate model on \"questions-words\" pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_questions_words(\n",
    "    \"data/questions-words.pickle\",\n",
    "    last_embedding_weights,\n",
    "    word_to_int,\n",
    "    words,\n",
    "    top_n=5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
