{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pickle\n",
    "import numpy as np\n",
    "rng_seed = 399\n",
    "np.random.seed(rng_seed)\n",
    "from os.path import join as join_path\n",
    "from utils import clean_sents\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, tokenizer_from_json\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and preprocess data\n",
    "We download the raw enwik9 data: http://mattmahoney.net/dc/textdata.html. Furthermore, we apply the wikifil.pl Perl script as specified in [Appendix A](http://mattmahoney.net/dc/textdata.html#appendixa) to parse the raw Wikipedia dump to clean text. We have made a minor justification to the last bit of the Perl script to further exclude the period \".\" character such that we can split the text into sentences later (from `tr/a-z/ /cs;` to `tr/a-z\\./ /cs;`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessing = False\n",
    "if data_preprocessing:\n",
    "    \n",
    "    # Download the raw data\n",
    "    !wget -c http://mattmahoney.net/dc/enwik9.zip -P data\n",
    "    !unzip data/enwik9.zip -d data\n",
    "\n",
    "    # A raw Wikipedia dump contains a lot of HTML / XML data.\n",
    "    # We pre-process it with the wikifil.pl script\n",
    "    # (originally developed by Matt Mahoney, and can be found on his website).\n",
    "    !perl data/wikifil.pl data/enwik9 > data/fil9\n",
    "\n",
    "    # Sanity checking first words of new file\n",
    "    !head -c 2000 data/fil9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "# ----------------\n",
    "data_dir = 'data'\n",
    "fil9_data_path = join_path(data_dir, 'fil9')\n",
    "fil9_data_tokenizer_config_path = join_path(data_dir, 'fil9-tokenizer.json')\n",
    "fil9_data_sentences_path = join_path(data_dir, 'fil9-sents.p')\n",
    "fil9_data_sequences_path = join_path(data_dir, 'fil9-seqs.p')\n",
    "# ----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean dataset\n",
    "To clean the dataset, we first use `sent_tokenize` from [NLTK](https://www.nltk.org/api/nltk.tokenize.html) to split the dataset into sentences. It is more convenient to use sentences instead of a big text, due to computational restrictions. To further clean the sentences, we convert the words to lowercase, apply lemmatization, filter out stopwords and remove words that are one character long.  \n",
    "\n",
    "**TODO**: Elaborate on lemmatization, stopwords etc. Why are they used? This also applies to other terms later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open(fil9_data_path, 'r') as file:\n",
    "    fil9_content = file.read()\n",
    "fil9_sents = sent_tokenize(fil9_content)\n",
    "\n",
    "# Remove stop words and lemmatize\n",
    "fil9_sents = clean_sents(fil9_sents, verbose=True)\n",
    "fil9_sents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fil9 sentences to file\n",
    "with open(fil9_data_sentences_path, 'wb') as file:\n",
    "    pickle.dump(fil9_sents, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary from texts\n",
    "fit_tokenizer = True\n",
    "if fit_tokenizer:\n",
    "    print('Creating vocabulary...')\n",
    "    \n",
    "    # Here we set the filters to empty string and lower case to false\n",
    "    # because we have already performed the nessecary preprocessing steps.\n",
    "    tokenizer = Tokenizer(filters='', lower=False)\n",
    "    tokenizer.fit_on_texts(tqdm(fil9_sents, unit='text'))\n",
    "\n",
    "    # Save to file\n",
    "    with open(fil9_data_tokenizer_config_path, 'w') as file:\n",
    "        file.write(tokenizer.to_json())\n",
    "    print('Done!')\n",
    "else:\n",
    "    print('Reading vocabulary...')\n",
    "    \n",
    "    # Read tokenizer from file\n",
    "    with open(fil9_data_tokenizer_config_path, 'r') as file:\n",
    "        tokenizer = tokenizer_from_json(file.read())\n",
    "    print('Done!')\n",
    "\n",
    "vocab_size = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fil9_seqs = tokenizer.texts_to_sequences(tqdm(fil9_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "tokenizer.word_index['man'], tokenizer.word_index['woman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write fil9 sequences to file\n",
    "with open(fil9_data_sequences_path, 'wb') as file:\n",
    "    pickle.dump(fil9_seqs, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
