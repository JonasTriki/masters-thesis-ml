{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/triki/.local/share/virtualenvs/code-gd-a4ILK/lib/python3.7/site-packages/tqdm/std.py:666: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "from os.path import join as join_path\n",
    "import numpy as np\n",
    "rng_seed = 368\n",
    "np.random.seed(rng_seed) # Random seed for reproducibility\n",
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Spacy (for language detection)\n",
    "# -----\n",
    "# To install:\n",
    "# !pip install spacy\n",
    "# !pip install scispacy\n",
    "# !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz\n",
    "# !pip install spacy-langdetect\n",
    "import scispacy\n",
    "import spacy\n",
    "import en_core_sci_lg # Biomedical word embeddings\n",
    "from spacy_langdetect import LanguageDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COVID.DATA.LIC.AGMT.pdf cord-19-data.csv        metadata.csv\r\n",
      "\u001b[1m\u001b[36mbiorxiv_medrxiv\u001b[m\u001b[m         \u001b[1m\u001b[36mcustom_license\u001b[m\u001b[m          metadata.readme\r\n",
      "\u001b[1m\u001b[36mcomm_use_subset\u001b[m\u001b[m         json_schema.txt         \u001b[1m\u001b[36mnoncomm_use_subset\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class CORD19Data():\n",
    "    '''\n",
    "    TODO: Docs\n",
    "    '''\n",
    "    def __init__(self, data_dir: str):\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "        # Initialize NLP model\n",
    "        self.nlp = en_core_sci_lg.load(disable=[\"tagger\", \"ner\"])\n",
    "        self.nlp.max_length = 2000000\n",
    "        self.nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\n",
    "        self.nlp_words_to_check = 100\n",
    "    \n",
    "    def _load_metadata(self) -> pd.DataFrame:\n",
    "        '''\n",
    "        TODO: Docs\n",
    "        '''\n",
    "        print('Loading metadata...')\n",
    "        cord_metadata_df = pd.read_csv(join_path(self.data_dir, 'metadata.csv'), dtype={\n",
    "            'pubmed_id': str,\n",
    "            'Microsoft Academic Paper ID': str, \n",
    "            'doi': str\n",
    "        })\n",
    "        print('Done!')\n",
    "        return cord_metadata_df\n",
    "\n",
    "    def _parse_json_article(self, article_path: str) -> tuple:\n",
    "        '''Parses a CORD-19 JSON article\n",
    "\n",
    "        Args:\n",
    "            article_path: JSON article path to parse\n",
    "\n",
    "        Returns:\n",
    "            TODO\n",
    "        '''\n",
    "        with open(article_path, 'r') as file:\n",
    "            content = json.load(file)\n",
    "\n",
    "            # Extract information\n",
    "            paper_id = content['paper_id']\n",
    "            abstract = []\n",
    "            body_text = []\n",
    "\n",
    "            # Abstract\n",
    "            for item in content['abstract']:\n",
    "                abstract.append(item['text'])\n",
    "\n",
    "            # Body text\n",
    "            for item in content['body_text']:\n",
    "                body_text.append(item['text'])\n",
    "\n",
    "            return paper_id, '\\n'.join(abstract), '\\n'.join(body_text)\n",
    "    \n",
    "    def _parse_articles(self) -> pd.DataFrame:\n",
    "        '''\n",
    "        TODO: Docs\n",
    "        '''\n",
    "        print('Parsing JSON articles...')\n",
    "        all_cord_article_paths = glob.glob(f'{self.data_dir}/**/*.json', recursive=True)\n",
    "        \n",
    "        # Initialize DataFrame dictionary\n",
    "        cord_articles_dict = {'paper_id': [], 'abstract': [], 'body_text': []}\n",
    "        for i, article_path in enumerate(tqdm(all_cord_article_paths, unit='article')):\n",
    "            paper_id, abstract, body_text = self._parse_json_article(article_path)\n",
    "            cord_articles_dict['paper_id'].append(paper_id)\n",
    "            cord_articles_dict['abstract'].append(abstract)\n",
    "            cord_articles_dict['body_text'].append(body_text)\n",
    "\n",
    "        df = pd.DataFrame(cord_articles_dict)\n",
    "        print('Done!')\n",
    "        return df\n",
    "    \n",
    "    def _merge_metadata_articles(self, metadata_df: pd.DataFrame, articles_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        '''\n",
    "        TODO: Docs\n",
    "        '''\n",
    "        print('Merging DataFrames...')\n",
    "        df = pd.merge(articles_df, metadata_df, left_on='paper_id', right_on='sha', how='left')\n",
    "        df = df.drop(['sha', 'abstract_y'], axis=1)\n",
    "        df = df.rename(columns = {'abstract_x': 'abstract', 'source_x': 'source'})\n",
    "        \n",
    "        print('Done!')\n",
    "        return df\n",
    "\n",
    "    def _exlude_non_metadata_articles(self, df: pd.DataFrame):\n",
    "        '''\n",
    "        TODO: Docs\n",
    "        '''\n",
    "        print('Excluding articles without metadata...')\n",
    "        df = df[df.full_text_file.notna()]\n",
    "        \n",
    "        print('Done!')\n",
    "        return df\n",
    "\n",
    "    def _remove_duplicates(self, df: pd.DataFrame):\n",
    "        '''\n",
    "        TODO: Docs\n",
    "        '''\n",
    "        print('Removing duplicates...')\n",
    "        df.drop_duplicates(['abstract', 'body_text'], inplace=True)\n",
    "\n",
    "        print('Done!')\n",
    "        return df\n",
    "    \n",
    "    def _extract_language(self, text: str) -> str:\n",
    "        '''\n",
    "        TODO: Docs\n",
    "        '''\n",
    "        # Extract language using spaCy\n",
    "        text_first_words = ' '.join(text.split(maxsplit=self.nlp_words_to_check)[:self.nlp_words_to_check])\n",
    "        lang = self.nlp(text_first_words)._.language['language']\n",
    "        \n",
    "        return lang\n",
    "    \n",
    "    def _perform_lang_detection(self, df: pd.DataFrame):\n",
    "        '''\n",
    "        TODO: Docs\n",
    "        '''\n",
    "        print('Performing language detection...')\n",
    "        \n",
    "        # Extract language\n",
    "        df['language'] = df.body_text.progress_apply(self._extract_language)\n",
    "\n",
    "        print('Done!')\n",
    "        return df\n",
    "    \n",
    "    def _save_to_file(self, df: pd.DataFrame, filename: str):\n",
    "        '''\n",
    "        TODO: Docs\n",
    "        '''\n",
    "        print('Saving to file...')\n",
    "        df.to_csv(filename, index=False)\n",
    "        print('Done!')\n",
    "    \n",
    "    def process_data(self, save_to_filename: str = None):\n",
    "        '''Processes the CORD-19 data.\n",
    "        \n",
    "        Loads and pre-processes CORD-19 data in specified data directory.\n",
    "        We take inspiration from/follow Daniel Wolffram's \"CORD-19: Create Dataframe\" Notebook\n",
    "        - https://www.kaggle.com/danielwolffram/cord-19-create-dataframe\n",
    "        \n",
    "        Args:\n",
    "            save_to_filename: Where to save the data after processing. Default: False\n",
    "        '''\n",
    "        # Perform pre-processing\n",
    "        metadata_df = self._load_metadata()\n",
    "        articles_df = self._parse_articles()\n",
    "        df = self._merge_metadata_articles(metadata_df, articles_df)\n",
    "        df = self._exlude_non_metadata_articles(df)\n",
    "        df = self._remove_duplicates(df)\n",
    "        df = self._perform_lang_detection(df)\n",
    "        \n",
    "        if save_to_filename != None:\n",
    "            self._save_to_file(df, save_to_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata...\n",
      "Done!\n",
      "Parsing JSON articles...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d96cc7dc4f94bc68b4ed7af7e0c4f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=33375.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done!\n",
      "Merging DataFrames...\n",
      "Done!\n",
      "Excluding articles without metadata...\n",
      "Done!\n",
      "Removing duplicates...\n",
      "Done!\n",
      "Performing language detection...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e97249f781fa485a916ca4243480ca30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=30184.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done!\n",
      "Saving to file...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "cord_data_dir = 'data'\n",
    "cord_data_path = join_path(cord_data_dir, 'cord-19-data.csv')\n",
    "CORD19Data(cord_data_dir).process_data(cord_data_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
