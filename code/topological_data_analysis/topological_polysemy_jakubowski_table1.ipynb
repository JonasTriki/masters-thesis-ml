{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from os import makedirs\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import joblib\n",
    "from nltk.corpus import wordnet as wn\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "rng_seed = 399\n",
    "np.random.seed(rng_seed)\n",
    "\n",
    "# Directory constants\n",
    "topological_data_analysis_data_dir = \"data\"\n",
    "root_code_dir = \"..\"\n",
    "output_dir = join(root_code_dir, \"output\")\n",
    "word2vec_training_dir = join(output_dir, \"word2vec_training\")\n",
    "topological_polysemy_experimentation_dir = join(\n",
    "    output_dir, \"topological_polysemy_experimentation\"\n",
    ")\n",
    "word_embeddings_data_dir = join(root_code_dir, \"word_embeddings\", \"data\")\n",
    "\n",
    "# Extend sys path for importing custom Python files\n",
    "import sys\n",
    "\n",
    "sys.path.append(root_code_dir)\n",
    "\n",
    "from word_embeddings.word2vec import load_model_training_output  # noqa: E402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SemEval data\n",
    "semeval_2010_14_word_senses = joblib.load(\n",
    "    join(topological_data_analysis_data_dir, \"semeval_2010_14_word_senses.joblib\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "semeval_target_word_tps_scores = {}\n",
    "semeval_target_words = np.array(list(semeval_2010_14_word_senses[\"all\"].keys()))\n",
    "semeval_gs_clusters = np.array(list(semeval_2010_14_word_senses[\"all\"].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "tps_neighbourhood_sizes = [10, 40, 50, 60, 100]\n",
    "tps_word_embeddings_names = [\n",
    "    \"enwiki\",\n",
    "    \"semeval_2010_task_14\",\n",
    "    \"fasttext_cc_300d\",\n",
    "    \"glove_cc_840b_300d\",\n",
    "    \"google_news_3m\",\n",
    "]\n",
    "tps_word_embeddings_model_names = [\n",
    "    \"enwiki\",\n",
    "    \"semeval_2010_task_14\",\n",
    "    \"cc.en.300.vec\",\n",
    "    \"glove.840B.300d\",\n",
    "    \"GoogleNews-vectors-negative300\",\n",
    "]\n",
    "tps_word_embeddings_is_external = [False, False, True, True, True]\n",
    "tps_word_embeddings_paths = [\n",
    "    join(word2vec_training_dir, \"word2vec_enwiki_jan_2021_word2phrase\"),\n",
    "    join(word2vec_training_dir, \"word2vec_semeval_2010_task_14\"),\n",
    "    join(word_embeddings_data_dir, \"fastText\"),\n",
    "    join(word_embeddings_data_dir, \"GloVe\"),\n",
    "    join(word_embeddings_data_dir, \"GoogleNews\"),\n",
    "]\n",
    "tps_vs_gs_key = \"TPS_n vs. GS\"\n",
    "tps_vs_synsets_key = \"TPS_n vs. synsets\"\n",
    "tps_vs_frequency_key = \"TPS_n vs. frequency\"\n",
    "num_top_k_words_frequencies = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tables\n",
    "tps_experiment_table_dicts = []\n",
    "for (\n",
    "    word_embeddings_name,\n",
    "    word_embeddings_model_name,\n",
    "    word_embeddings_path,\n",
    "    word_embeddings_is_external,\n",
    ") in zip(\n",
    "    tps_word_embeddings_names,\n",
    "    tps_word_embeddings_model_names,\n",
    "    tps_word_embeddings_paths,\n",
    "    tps_word_embeddings_is_external,\n",
    "):\n",
    "    # Load data\n",
    "    if word_embeddings_is_external:\n",
    "        model_words_filepath = join(\n",
    "            word_embeddings_path, f\"{word_embeddings_model_name}_words.txt\"\n",
    "        )\n",
    "        with open(model_words_filepath, \"r\") as words_file:\n",
    "            model_words = np.array(words_file.read().split(\"\\n\"))\n",
    "        word_to_int = {word: i for i, word in enumerate(model_words)}\n",
    "    else:\n",
    "        print(f\"Loading {word_embeddings_name} word embeddings...\")\n",
    "        w2v_training_output = load_model_training_output(\n",
    "            model_training_output_dir=word_embeddings_path,\n",
    "            model_name=\"word2vec\",\n",
    "            dataset_name=word_embeddings_model_name,\n",
    "        )\n",
    "        model_words = w2v_training_output[\"words\"]\n",
    "        word_to_int = w2v_training_output[\"word_to_int\"]\n",
    "        word_counts = w2v_training_output[\"word_counts\"]\n",
    "        print(\"Done!\")\n",
    "\n",
    "    # Filter SemEval words in vocabulary\n",
    "    semeval_target_words_in_vocab_filter = [\n",
    "        i for i, word in enumerate(semeval_target_words) if word in word_to_int\n",
    "    ]\n",
    "    semeval_target_words_in_vocab = semeval_target_words[\n",
    "        semeval_target_words_in_vocab_filter\n",
    "    ]\n",
    "    semeval_gs_clusters_in_vocab = semeval_gs_clusters[\n",
    "        semeval_target_words_in_vocab_filter\n",
    "    ]\n",
    "    num_semeval_words = len(semeval_gs_clusters_in_vocab)\n",
    "\n",
    "    # Find words in vocabulary that have synsets in Wordnet\n",
    "    wordnet_synsets_words_in_vocab_meanings = []\n",
    "    print(\"Find words in vocabulary that have synsets in Wordnet...\")\n",
    "    for word in tqdm(model_words):\n",
    "        num_synsets_word = len(wn.synsets(word))\n",
    "        if num_synsets_word > 0:\n",
    "            wordnet_synsets_words_in_vocab_meanings.append(num_synsets_word)\n",
    "\n",
    "    result_dict: dict = {\n",
    "        \"n\": tps_neighbourhood_sizes,\n",
    "        tps_vs_gs_key: [],\n",
    "        tps_vs_synsets_key: [],\n",
    "    }\n",
    "    if not word_embeddings_is_external:\n",
    "        result_dict[tps_vs_frequency_key] = []\n",
    "\n",
    "    # Fill in dictionary\n",
    "    for n_size in tps_neighbourhood_sizes:\n",
    "\n",
    "        # TPS_n score vs. GS\n",
    "        tps_scores_semeval = np.load(\n",
    "            join(topological_polysemy_experimentation_dir, f\"tps_{n_size}_vs_gs.npy\")\n",
    "        )\n",
    "        tps_score_vs_gs_correlation, _ = pearsonr(\n",
    "            x=tps_scores_semeval, y=semeval_gs_clusters_in_vocab\n",
    "        )\n",
    "        result_dict[tps_vs_gs_key].append(tps_score_vs_gs_correlation)\n",
    "\n",
    "        # TPS_n score vs. Synsets\n",
    "        tps_scores_wordnet_synsets = np.load(\n",
    "            join(\n",
    "                topological_polysemy_experimentation_dir, f\"tps_{n_size}_vs_synsets.npy\"\n",
    "            )\n",
    "        )\n",
    "        tps_score_vs_wordnet_synsets_correlation, _ = pearsonr(\n",
    "            x=tps_scores_wordnet_synsets, y=wordnet_synsets_words_in_vocab_meanings\n",
    "        )\n",
    "        result_dict[tps_vs_synsets_key].append(tps_score_vs_wordnet_synsets_correlation)\n",
    "\n",
    "        # TPS_n score vs. frequency\n",
    "        if not word_embeddings_is_external:\n",
    "            tps_score_vs_word_frequency_correlation, _ = pearsonr(\n",
    "                x=tps_score_word_frequencies,\n",
    "                y=word_counts[:num_top_k_words_frequencies],\n",
    "            )\n",
    "            result_dict[tps_vs_frequency_key].append(\n",
    "                tps_score_vs_word_frequency_correlation\n",
    "            )\n",
    "    tps_experiment_table_dicts.append(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from os import makedirs\n",
    "from os.path import join\n",
    "import joblib\n",
    "import numpy as np\n",
    "rng_seed = 399\n",
    "np.random.seed(rng_seed)\n",
    "from scipy.stats import pearsonr\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import gudhi as gd\n",
    "from gudhi.wasserstein import wasserstein_distance\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import plotly.offline as pyo\n",
    "pyo.init_notebook_mode()\n",
    "\n",
    "# Directory constants\n",
    "topological_data_analysis_data_dir = \"data\"\n",
    "root_code_dir = \"..\"\n",
    "output_dir = join(root_code_dir, \"output\")\n",
    "word2vec_training_dir = join(output_dir, \"word2vec_training\")\n",
    "word2vec_ann_indices_dir = join(output_dir, \"word2vec_ann_indices\")\n",
    "word2vec_cluster_analysis_dir = join(output_dir, \"word2vec_cluster_analysis\")\n",
    "\n",
    "# Extend sys path for importing custom Python files\n",
    "import sys\n",
    "sys.path.append(root_code_dir)\n",
    "\n",
    "from utils import get_model_checkpoint_filepaths, pairwise_cosine_distances, words_to_vectors\n",
    "from word_embeddings.word2vec import load_model_training_output\n",
    "from vis_utils import plot_word_vectors\n",
    "from topological_data_analysis.tda_utils import plot_persistence_diagram\n",
    "from topological_polysemy import tps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load output from training word2vec\n",
    "w2v_training_output = load_model_training_output(\n",
    "    model_training_output_dir=join(word2vec_training_dir, \"word2vec_enwiki_jan_2021_word2phrase\"),\n",
    "    model_name=\"word2vec\",\n",
    "    dataset_name=\"enwiki\",\n",
    "    return_normalized_embeddings=True,\n",
    "    return_scann_instance=True\n",
    ")\n",
    "last_embedding_weights_normalized = w2v_training_output[\"last_embedding_weights_normalized\"]\n",
    "last_embedding_weights_scann_instance = w2v_training_output[\"last_embedding_weights_scann_instance\"]\n",
    "words = w2v_training_output[\"words\"]\n",
    "word_to_int = w2v_training_output[\"word_to_int\"]\n",
    "word_counts = w2v_training_output[\"word_counts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SemEval data\n",
    "semeval_2010_14_word_senses = joblib.load(\n",
    "    join(topological_data_analysis_data_dir, \"semeval_2010_14_word_senses.joblib\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topolocial polysemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tps_neighbourhood_sizes = [10, 40, 50, 60, 100, 150, 200, 500, 1000, 1500]\n",
    "table_1_dict = {\n",
    "    \"n\": tps_neighbourhood_sizes,\n",
    "    \"TPS_n vs. GS\": [],\n",
    "    \"TPS_n vs. synsets\": [],\n",
    "    \"TPS_n vs. frequency\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPS for 100 SemEval target words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "semeval_target_word_tps_scores = {}\n",
    "\n",
    "semeval_target_words = np.array(list(semeval_2010_14_word_senses[\"all\"].keys()))\n",
    "semeval_target_words_in_vocab_filter = [\n",
    "    i for i, word in enumerate(semeval_target_words) if word in word_to_int\n",
    "]\n",
    "semeval_target_words_in_vocab = semeval_target_words[\n",
    "    semeval_target_words_in_vocab_filter\n",
    "]\n",
    "semeval_gs_clusters = np.array(list(semeval_2010_14_word_senses[\"all\"].values()))\n",
    "semeval_gs_clusters_in_vocab = semeval_gs_clusters[semeval_target_words_in_vocab_filter]\n",
    "\n",
    "num_semeval_words = len(semeval_gs_clusters_in_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute TPS for 100 SemEval target words\n",
    "semeval_target_word_tps_scores = {}\n",
    "for tps_neighbourhood_size in tps_neighbourhood_sizes:\n",
    "    print(f\"Neighbourhood size: {tps_neighbourhood_size}\")\n",
    "    \n",
    "    # Compute TPS scores\n",
    "    semeval_tps_scores = []\n",
    "    for semeval_target_word, semeval_target_word_clusters in tqdm(\n",
    "        zip(semeval_target_words_in_vocab, semeval_gs_clusters_in_vocab),\n",
    "        total=num_semeval_words,\n",
    "    ):\n",
    "        tps_score = tps(\n",
    "            target_word=semeval_target_word,\n",
    "            word_to_int=word_to_int,\n",
    "            neighbourhood_size=tps_neighbourhood_size,\n",
    "            word_embeddings_normalized=last_embedding_weights_normalized,\n",
    "            ann_instance=last_embedding_weights_scann_instance\n",
    "        )\n",
    "        semeval_tps_scores.append(tps_score)\n",
    "        \n",
    "    # Compute correlation\n",
    "    semeval_tps_score_gs_corr, semeval_tps_score_gs_corr_p_value = pearsonr(\n",
    "        x=semeval_tps_scores,\n",
    "        y=semeval_gs_clusters_in_vocab\n",
    "    )\n",
    "    \n",
    "    # Set result\n",
    "    semeval_target_word_tps_scores[tps_neighbourhood_size] = {\n",
    "        \"tps_scores\": semeval_tps_scores,\n",
    "        \"gs_tps_correlation\": semeval_tps_score_gs_corr\n",
    "    }\n",
    "    table_1_dict[\"TPS_n vs. GS\"].append(semeval_tps_score_gs_corr)\n",
    "    \n",
    "    # Plot TPS scores to GS\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.scatter(\n",
    "        x=semeval_tps_scores,\n",
    "        y=semeval_gs_clusters_in_vocab\n",
    "    )\n",
    "    plt.xlabel(\"TPS\")\n",
    "    plt.ylabel(\"Clusters in GS\")\n",
    "    plt.title(f\"Correlation: {semeval_tps_score_gs_corr:.5f}, p-value: {semeval_tps_score_gs_corr_p_value:.5f}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPS for Wordnet synsets that are in vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find words in vocabulary that have synsets in Wordnet\n",
    "wordnet_synsets_in_vocab = {} \n",
    "for word in tqdm(words):\n",
    "    num_synsets_word = len(wn.synsets(word))\n",
    "    if num_synsets_word > 0:\n",
    "        wordnet_synsets_in_vocab[word] = num_synsets_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_synsets_words_in_vocab = list(wordnet_synsets_in_vocab.keys())\n",
    "wordnet_synsets_words_in_vocab_meanings = list(wordnet_synsets_in_vocab.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for tps_neighbourhood_size in tps_neighbourhood_sizes:\n",
    "    print(f\"Neighbourhood size: {tps_neighbourhood_size}\")\n",
    "    \n",
    "    # Compute TPS scores\n",
    "    tps_scores = []\n",
    "    for word in tqdm(wordnet_synsets_words_in_vocab):\n",
    "        tps_score = tps(\n",
    "            target_word=word,\n",
    "            word_embeddings=last_embedding_weights,\n",
    "            words_vocabulary=None,\n",
    "            word_to_int=word_to_int,\n",
    "            neighbourhood_size=tps_neighbourhood_size,\n",
    "            word_embeddings_normalized=last_embedding_weights_normalized,\n",
    "            word_embeddings_pairwise_dists=None,\n",
    "            ann_instance=last_embedding_weights_scann_instance\n",
    "        )\n",
    "        tps_scores.append(tps_score)\n",
    "        \n",
    "    # Compute correlation\n",
    "    tps_score_synsets_corr, _ = pearsonr(\n",
    "        x=tps_scores,\n",
    "        y=wordnet_synsets_words_in_vocab_meanings\n",
    "    )\n",
    "    table_1_dict[\"TPS_n vs. synsets\"].append(tps_score_synsets_corr)\n",
    "    \n",
    "    # Plot TPS scores to Wordnet synsets\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.scatter(\n",
    "        x=tps_scores,\n",
    "        y=wordnet_synsets_words_in_vocab_meanings\n",
    "    )\n",
    "    plt.xlabel(\"TPS\")\n",
    "    plt.ylabel(\"Synsets in Wordnet\")\n",
    "    plt.title(f\"Correlation: {tps_score_synsets_corr:.5f}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPS for top 10k words (vs. word frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_k_words = 10000\n",
    "top_k_frequencies = word_counts[:num_top_k_words]\n",
    "table_1_dict[\"TPS_n vs. frequency\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for tps_neighbourhood_size in tps_neighbourhood_sizes:\n",
    "    print(f\"Neighbourhood size: {tps_neighbourhood_size}\")\n",
    "    \n",
    "    # Compute TPS scores\n",
    "    tps_scores = []\n",
    "    for word in tqdm(words[:num_top_k_words]):\n",
    "        tps_score = tps( \n",
    "            target_word=word,\n",
    "            word_embeddings=last_embedding_weights,\n",
    "            words_vocabulary=None,\n",
    "            word_to_int=word_to_int,\n",
    "            neighbourhood_size=tps_neighbourhood_size,\n",
    "            word_embeddings_normalized=last_embedding_weights_normalized,\n",
    "            word_embeddings_pairwise_dists=None,\n",
    "            ann_instance=last_embedding_weights_scann_instance\n",
    "        )\n",
    "        tps_scores.append(tps_score)\n",
    "\n",
    "    # Compute correlation\n",
    "    tps_score_frequency_corr, _ = pearsonr(\n",
    "        x=tps_scores,\n",
    "        y=top_k_frequencies\n",
    "    )\n",
    "    table_1_dict[\"TPS_n vs. frequency\"].append(tps_score_frequency_corr)\n",
    "    \n",
    "    # Plot TPS scores to word frequencies\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.scatter(\n",
    "        x=tps_scores,\n",
    "        y=top_k_frequencies\n",
    "    )\n",
    "    plt.xlabel(\"TPS\")\n",
    "    plt.ylabel(\"Word frequency\")\n",
    "    plt.title(f\"Correlation: {tps_score_frequency_corr:.5f}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show \"Table 1\"\n",
    "table_1_df = pd.DataFrame(table_1_dict)\n",
    "table_1_df.set_index(\"n\", inplace=True)\n",
    "table_1_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
