{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-horizontal",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Callable\n",
    "\n",
    "rng_seed = 399\n",
    "np.random.seed(rng_seed)\n",
    "import joblib\n",
    "from tqdm.auto import tqdm\n",
    "from gudhi import plot_persistence_diagram, bottleneck_distance\n",
    "from gudhi.wasserstein import wasserstein_distance\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.manifold import Isomap\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering\n",
    "\n",
    "import plotly.offline as pyo\n",
    "\n",
    "pyo.init_notebook_mode()\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Directory constants\n",
    "topological_data_analysis_data_dir = \"data\"\n",
    "topological_data_analysis_custom_data_dir = \"custom_data\"\n",
    "root_code_dir = \"..\"\n",
    "output_dir = join(root_code_dir, \"output\")\n",
    "word2vec_training_dir = join(output_dir, \"word2vec_training\")\n",
    "word2vec_ann_indices_dir = join(output_dir, \"word2vec_ann_indices\")\n",
    "word2vec_cluster_analysis_dir = join(output_dir, \"word2vec_cluster_analysis\")\n",
    "\n",
    "# Extend sys path for importing custom Python files\n",
    "import sys\n",
    "\n",
    "sys.path.append(root_code_dir)\n",
    "\n",
    "from utils import (\n",
    "    get_model_checkpoint_filepaths,\n",
    "    pairwise_cosine_distances,\n",
    "    words_to_vectors,\n",
    "    normalize_array,\n",
    ")\n",
    "from word_embeddings.word2vec import load_model_training_output\n",
    "from vis_utils import plot_word_vectors\n",
    "from topological_data_analysis.tda_utils import (\n",
    "    generate_points_in_spheres,\n",
    ")\n",
    "from topological_data_analysis.topological_polysemy import tps, tps_point_cloud\n",
    "from topological_data_analysis.geometric_anomaly_detection import (\n",
    "    GeometricAnomalyDetection,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-perfume",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-modem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "tps_neighbourhood_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-queen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load output from training word2vec\n",
    "w2v_training_output = load_model_training_output(\n",
    "    model_training_output_dir=join(\n",
    "        word2vec_training_dir, \"word2vec_enwiki_jan_2021_word2phrase\"\n",
    "    ),\n",
    "    model_name=\"word2vec\",\n",
    "    dataset_name=\"enwiki\",\n",
    "    return_normalized_embeddings=True,\n",
    "    return_annoy_index=True,\n",
    "    annoy_index_prefault=True,\n",
    ")\n",
    "last_embedding_weights = w2v_training_output[\"last_embedding_weights\"]\n",
    "last_embedding_weights_normalized = w2v_training_output[\n",
    "    \"last_embedding_weights_normalized\"\n",
    "]\n",
    "last_embedding_weights_annoy_index = w2v_training_output[\n",
    "    \"last_embedding_weights_annoy_index\"\n",
    "]\n",
    "words = w2v_training_output[\"words\"]\n",
    "word_to_int = w2v_training_output[\"word_to_int\"]\n",
    "word_counts = w2v_training_output[\"word_counts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-programming",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SemEval data\n",
    "semeval_2010_14_word_senses = joblib.load(\n",
    "    join(topological_data_analysis_data_dir, \"semeval_2010_14_word_senses.joblib\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-mambo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use SemEval words in vocabulary\n",
    "semeval_target_word_tps_scores = {}\n",
    "semeval_target_words = np.array(list(semeval_2010_14_word_senses[\"all\"].keys()))\n",
    "semeval_target_words_in_vocab_filter = [\n",
    "    i for i, word in enumerate(semeval_target_words) if word in word_to_int\n",
    "]\n",
    "semeval_target_words_in_vocab = semeval_target_words[\n",
    "    semeval_target_words_in_vocab_filter\n",
    "]\n",
    "semeval_gs_clusters = np.array(list(semeval_2010_14_word_senses[\"all\"].values()))\n",
    "semeval_gs_clusters_in_vocab = semeval_gs_clusters[semeval_target_words_in_vocab_filter]\n",
    "\n",
    "num_semeval_words = len(semeval_gs_clusters_in_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-norman",
   "metadata": {},
   "source": [
    "# Compute TPS/persistence diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-favor",
   "metadata": {},
   "outputs": [],
   "source": [
    "tps_pds = []\n",
    "tps_scores = []\n",
    "for word, senses in tqdm(\n",
    "    zip(semeval_target_words_in_vocab, semeval_gs_clusters_in_vocab),\n",
    "    total=num_semeval_words,\n",
    "):\n",
    "    tps_score, tps_pd = tps(\n",
    "        target_word=word,\n",
    "        word_to_int=word_to_int,\n",
    "        neighbourhood_size=tps_neighbourhood_size,\n",
    "        word_embeddings_normalized=last_embedding_weights_normalized,\n",
    "        annoy_index=last_embedding_weights_annoy_index,\n",
    "        return_persistence_diagram=True,\n",
    "    )\n",
    "    tps_scores.append(tps_score)\n",
    "    tps_pds.append(tps_pd)\n",
    "tps_scores = np.array(tps_scores)\n",
    "tps_pds = np.array(tps_pds, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-amateur",
   "metadata": {},
   "outputs": [],
   "source": [
    "tps_scores_sorted_indices = np.argsort(tps_scores)[::-1]\n",
    "tps_scores_sorted = tps_scores[tps_scores_sorted_indices]\n",
    "tps_pds_sorted = tps_pds[tps_scores_sorted_indices]\n",
    "semeval_target_words_in_vocab_sorted = semeval_target_words_in_vocab[\n",
    "    tps_scores_sorted_indices\n",
    "]\n",
    "semeval_gs_clusters_in_vocab_sorted = semeval_gs_clusters_in_vocab[\n",
    "    tps_scores_sorted_indices\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-emission",
   "metadata": {},
   "source": [
    "# Visulize PDs with high/low TPS_50 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-trustee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(6 * 3, 5 * 2))\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        ax = axes[i, j]\n",
    "        if i == 0:\n",
    "            tps_idx = j\n",
    "        else:\n",
    "            tps_idx = num_semeval_words - 3 + j\n",
    "        word = semeval_target_words_in_vocab_sorted[tps_idx]\n",
    "        word_tps = tps_scores_sorted[tps_idx]\n",
    "\n",
    "        plot_persistence_diagram(tps_pds_sorted[tps_idx], axes=ax)\n",
    "        ax.set_title(f\"PD of '{word}' (TPS_{tps_neighbourhood_size}={word_tps:.5f})\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-rapid",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd_to_matrix(pd: np.ndarray, target_dim: int, include_inf: bool) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    TODO: Docs\n",
    "    \"\"\"\n",
    "    result_mat = []\n",
    "    for dim, (birth, death) in pd:\n",
    "        if dim != target_dim or (not include_inf and death == np.inf):\n",
    "            continue\n",
    "        result_mat.append([birth, death])\n",
    "    return np.array(result_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-phrase",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tps_pds_to_matrix(tps_pds: np.array, include_inf: bool = False) -> np.array:\n",
    "    \"\"\"\n",
    "    TODO: Docs\n",
    "    \"\"\"\n",
    "    result_mat = []\n",
    "    for tps_pd in tps_pds:\n",
    "        pd_mat = pd_to_matrix(pd=tps_pd, include_inf=include_inf, target_dim=0)\n",
    "        result_mat.append(pd_mat)\n",
    "    return np.array(result_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-beijing",
   "metadata": {},
   "source": [
    "# Clustering of TPS PDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-worcester",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distances_pds(\n",
    "    persistence_diagrams: np.ndarray, metric: Callable[[np.ndarray, np.ndarray], float], **kwargs: dict\n",
    ") -> np.array:\n",
    "    \"\"\"\n",
    "    TODO: Docs\n",
    "    \"\"\"\n",
    "    n = len(persistence_diagrams)\n",
    "    pairwise_dists = np.zeros((n, n))\n",
    "    for i in tqdm(range(n)):\n",
    "        for j in range(n):\n",
    "            pairwise_dists[i, j] = metric(\n",
    "                persistence_diagrams[i], persistence_diagrams[j], **kwargs\n",
    "            )\n",
    "    np.fill_diagonal(pairwise_dists, 0)\n",
    "\n",
    "    return pairwise_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-treatment",
   "metadata": {},
   "outputs": [],
   "source": [
    "tps_pds_data = tps_pds_to_matrix(tps_pds_sorted)\n",
    "tps_pds_data_with_inf = tps_pds_to_matrix(tps_pds_sorted, include_inf=True)\n",
    "tps_pds_data_pairwise_dists_euclidean = pairwise_distances(\n",
    "    tps_pds_data[:, :, 1], metric=\"euclidean\"\n",
    ")\n",
    "tps_pds_data_pairwise_dists_bottle = pairwise_distances_pds(\n",
    "    tps_pds_data, metric=bottleneck_distance\n",
    ")\n",
    "tps_pds_data_pairwise_dists_wasser = pairwise_distances_pds(\n",
    "    tps_pds_data, metric=wasserstein_distance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-effort",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for dist_metric_name, pairwise_dists in [\n",
    "    (\"Euclidean\", tps_pds_data_pairwise_dists_euclidean),\n",
    "    (\"Bottleneck\", tps_pds_data_pairwise_dists_bottle),\n",
    "    (\"Wasserstein\", tps_pds_data_pairwise_dists_wasser),\n",
    "]:\n",
    "    tps_pds_data_2d_mds = MDS(\n",
    "        n_components=2, dissimilarity=\"precomputed\", random_state=rng_seed\n",
    "    ).fit_transform(pairwise_dists)\n",
    "    tps_pds_data_2d_umap = UMAP(\n",
    "        n_components=2, n_neighbors=5, metric=\"precomputed\", random_state=rng_seed\n",
    "    ).fit_transform(pairwise_dists)\n",
    "    tps_pds_data_pred_labels = SpectralClustering(\n",
    "        n_clusters=2, affinity=\"precomputed\", random_state=rng_seed\n",
    "    ).fit_predict(pairwise_dists)\n",
    "    for embedding_name, embedding in [\n",
    "        (\"MDS\", tps_pds_data_2d_mds),\n",
    "        (\"UMAP\", tps_pds_data_2d_umap),\n",
    "    ]:\n",
    "        fig = px.scatter(\n",
    "            x=embedding[:, 0],\n",
    "            y=embedding[:, 1],\n",
    "            color=tps_scores_sorted,\n",
    "            symbol=[\n",
    "                \"circle\" if label == 0 else \"cross\"\n",
    "                for label in tps_pds_data_pred_labels\n",
    "            ],\n",
    "            hover_data={\"word\": semeval_target_words_in_vocab_sorted},\n",
    "            labels={\n",
    "                \"x\": f\"{embedding_name}1\",\n",
    "                \"y\": f\"{embedding_name}2\",\n",
    "                \"color\": \"TPS_50\",\n",
    "            },\n",
    "            title=f\"{dist_metric_name} distance with {embedding_name}\",\n",
    "        )\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-disclosure",
   "metadata": {},
   "source": [
    "# Clustering of cyclo-octane GAD PDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-vertical",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cyclo-octane data\n",
    "cyclo_octane_data = pd.read_csv(\n",
    "    join(topological_data_analysis_custom_data_dir, \"cyclo-octane.csv\"), header=None\n",
    ").values\n",
    "cyclo_octane_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "gad_instance = GeometricAnomalyDetection(cyclo_octane_data)\n",
    "gad_result = gad_instance.compute(\n",
    "    word_ints=None,\n",
    "    manifold_dimension=2,\n",
    "    annulus_inner_radius=0.25,\n",
    "    annulus_outer_radius=0.4,\n",
    "    tqdm_enabled=True,\n",
    "    return_annlus_persistence_diagrams=True,\n",
    ")\n",
    "annulus_pds = gad_result[\"annulus_pds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-jones",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector with point colors\n",
    "point_colors = np.empty(len(cyclo_octane_data), dtype=object)\n",
    "for i in range(len(cyclo_octane_data)):\n",
    "    for key in gad_result.keys():\n",
    "        if key == \"annulus_pds\":\n",
    "            continue\n",
    "        if i in gad_result[key]:\n",
    "            point_colors[i] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-causing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gad_pds_data_pairwise_dists_euclidean = pairwise_distances(\n",
    "#    annulus_pds[:, :, 1], metric=\"euclidean\"\n",
    "# )\n",
    "compute_pd_pairwise_dists = False\n",
    "if compute_pd_pairwise_dists:\n",
    "    gad_pds_data_pairwise_dists_bottle = pairwise_distances_pds(\n",
    "        annulus_pds, metric=bottleneck_distance\n",
    "    )\n",
    "    gad_pds_data_pairwise_dists_wasser = pairwise_distances_pds(\n",
    "        annulus_pds, metric=wasserstein_distance\n",
    "    )\n",
    "\n",
    "    # Save\n",
    "    np.save(\n",
    "        \"gad_pds_data_pairwise_dists_bottle.npy\", gad_pds_data_pairwise_dists_bottle\n",
    "    )\n",
    "    np.save(\n",
    "        \"gad_pds_data_pairwise_dists_wasser.npy\", gad_pds_data_pairwise_dists_wasser\n",
    "    )\n",
    "else:\n",
    "    gad_pds_data_pairwise_dists_bottle = np.load(\n",
    "        \"gad_pds_data_pairwise_dists_bottle.npy\"\n",
    "    )\n",
    "    gad_pds_data_pairwise_dists_wasser = np.load(\n",
    "        \"gad_pds_data_pairwise_dists_wasser.npy\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-truth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure bottleneck distance is symmetric\n",
    "gad_pds_data_pairwise_dists_bottle_sym = (\n",
    "    gad_pds_data_pairwise_dists_bottle + gad_pds_data_pairwise_dists_bottle.T\n",
    ") / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-african",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for dist_metric_name, pairwise_dists in [\n",
    "    # (\"Euclidean\", gad_pds_data_pairwise_dists_bottle),\n",
    "    (\"Bottleneck\", gad_pds_data_pairwise_dists_bottle_sym),\n",
    "    (\"Wasserstein\", gad_pds_data_pairwise_dists_wasser),\n",
    "]:\n",
    "    print(\"UMAP...\")\n",
    "    pds_data_3d_umap = UMAP(\n",
    "        n_components=3, n_neighbors=15, metric=\"precomputed\", random_state=rng_seed\n",
    "    ).fit_transform(pairwise_dists)\n",
    "    print(\"Done!\")\n",
    "    print(\"Isomap...\")\n",
    "    pds_data_3d_isomap = Isomap(n_components=3, metric=\"precomputed\").fit_transform(\n",
    "        pairwise_dists\n",
    "    )\n",
    "    print(\"Done!\")\n",
    "    # print(\"AgglomerativeClustering...\")\n",
    "    # pds_data_pred_labels = AgglomerativeClustering(\n",
    "    #    n_clusters=3, affinity=\"precomputed\", linkage=\"average\"\n",
    "    # ).fit_predict(pairwise_dists)\n",
    "    # print(\"Done!\")\n",
    "    for embedding_name, embedding in [\n",
    "        (\"UMAP\", pds_data_3d_umap),\n",
    "        (\"Isomap\", pds_data_3d_isomap),\n",
    "    ]:\n",
    "        fig = px.scatter_3d(\n",
    "            x=embedding[:, 0],\n",
    "            y=embedding[:, 1],\n",
    "            z=embedding[:, 2],\n",
    "            color=point_colors,\n",
    "            symbol=[\n",
    "                {\"P_bnd\": \"square\", \"P_int\": \"cross\", \"P_man\": \"circle\"}[label]\n",
    "                for label in point_colors\n",
    "            ],\n",
    "            # hover_data={\"word\": semeval_target_words_in_vocab_sorted},\n",
    "            labels={\n",
    "                \"x\": f\"{embedding_name}1\",\n",
    "                \"y\": f\"{embedding_name}2\",\n",
    "                \"z\": f\"{embedding_name}3\",\n",
    "                \"color\": \"Category\",\n",
    "            },\n",
    "            title=f\"{dist_metric_name} distance with {embedding_name}\",\n",
    "        )\n",
    "        fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
