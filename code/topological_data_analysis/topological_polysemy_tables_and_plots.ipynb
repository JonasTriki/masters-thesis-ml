{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"# Imports\\nfrom os import makedirs\\nfrom os import listdir\\nfrom os.path import join\\nimport pandas as pd\\nimport numpy as np\\nimport joblib\\nfrom nltk.corpus import wordnet as wn\\nfrom scipy.stats import pearsonr\\nfrom tqdm.auto import tqdm\\nfrom IPython.display import display, HTML\\n\\nrng_seed = 399\\nnp.random.seed(rng_seed)\\n\\n# Directory constants\\ntopological_data_analysis_data_dir = \\\"data\\\"\\nroot_code_dir = \\\"..\\\"\\noutput_dir = join(root_code_dir, \\\"output\\\")\\nword2vec_training_dir = join(output_dir, \\\"word2vec_training\\\")\\ntopological_polysemy_experimentation_dir = join(\\n    output_dir, \\\"topological_polysemy_experimentation\\\"\\n)\\nword_embeddings_data_dir = join(root_code_dir, \\\"word_embeddings\\\", \\\"data\\\")\\n\\n# Extend sys path for importing custom Python files\\nimport sys\\n\\nsys.path.append(root_code_dir)\\n\\nfrom word_embeddings.word2vec import load_model_training_output  # noqa: E402\";\n",
       "                var nbb_formatted_code = \"# Imports\\nfrom os import makedirs\\nfrom os import listdir\\nfrom os.path import join\\nimport pandas as pd\\nimport numpy as np\\nimport joblib\\nfrom nltk.corpus import wordnet as wn\\nfrom scipy.stats import pearsonr\\nfrom tqdm.auto import tqdm\\nfrom IPython.display import display, HTML\\n\\nrng_seed = 399\\nnp.random.seed(rng_seed)\\n\\n# Directory constants\\ntopological_data_analysis_data_dir = \\\"data\\\"\\nroot_code_dir = \\\"..\\\"\\noutput_dir = join(root_code_dir, \\\"output\\\")\\nword2vec_training_dir = join(output_dir, \\\"word2vec_training\\\")\\ntopological_polysemy_experimentation_dir = join(\\n    output_dir, \\\"topological_polysemy_experimentation\\\"\\n)\\nword_embeddings_data_dir = join(root_code_dir, \\\"word_embeddings\\\", \\\"data\\\")\\n\\n# Extend sys path for importing custom Python files\\nimport sys\\n\\nsys.path.append(root_code_dir)\\n\\nfrom word_embeddings.word2vec import load_model_training_output  # noqa: E402\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Imports\n",
    "from os import makedirs\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from nltk.corpus import wordnet as wn\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "rng_seed = 399\n",
    "np.random.seed(rng_seed)\n",
    "\n",
    "# Directory constants\n",
    "topological_data_analysis_data_dir = \"data\"\n",
    "root_code_dir = \"..\"\n",
    "output_dir = join(root_code_dir, \"output\")\n",
    "word2vec_training_dir = join(output_dir, \"word2vec_training\")\n",
    "topological_polysemy_experimentation_dir = join(\n",
    "    output_dir, \"topological_polysemy_experimentation\"\n",
    ")\n",
    "word_embeddings_data_dir = join(root_code_dir, \"word_embeddings\", \"data\")\n",
    "\n",
    "# Extend sys path for importing custom Python files\n",
    "import sys\n",
    "\n",
    "sys.path.append(root_code_dir)\n",
    "\n",
    "from word_embeddings.word2vec import load_model_training_output  # noqa: E402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# Load SemEval data\\nsemeval_2010_14_word_senses = joblib.load(\\n    join(topological_data_analysis_data_dir, \\\"semeval_2010_14_word_senses.joblib\\\")\\n)\";\n",
       "                var nbb_formatted_code = \"# Load SemEval data\\nsemeval_2010_14_word_senses = joblib.load(\\n    join(topological_data_analysis_data_dir, \\\"semeval_2010_14_word_senses.joblib\\\")\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load SemEval data\n",
    "semeval_2010_14_word_senses = joblib.load(\n",
    "    join(topological_data_analysis_data_dir, \"semeval_2010_14_word_senses.joblib\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# Prepare data\\nsemeval_target_word_tps_scores = {}\\nsemeval_target_words = np.array(list(semeval_2010_14_word_senses[\\\"all\\\"].keys()))\\nsemeval_gs_clusters = np.array(list(semeval_2010_14_word_senses[\\\"all\\\"].values()))\";\n",
       "                var nbb_formatted_code = \"# Prepare data\\nsemeval_target_word_tps_scores = {}\\nsemeval_target_words = np.array(list(semeval_2010_14_word_senses[\\\"all\\\"].keys()))\\nsemeval_gs_clusters = np.array(list(semeval_2010_14_word_senses[\\\"all\\\"].values()))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare data\n",
    "semeval_target_word_tps_scores = {}\n",
    "semeval_target_words = np.array(list(semeval_2010_14_word_senses[\"all\"].keys()))\n",
    "semeval_gs_clusters = np.array(list(semeval_2010_14_word_senses[\"all\"].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"# Constants\\ntps_neighbourhood_sizes = [10, 40, 50, 60, 100]\\ntps_word_embeddings_names = [\\n    \\\"enwiki\\\",\\n    \\\"semeval_2010_task_14\\\",\\n    \\\"fasttext_cc_300d\\\",\\n    \\\"glove_cc_840b_300d\\\",\\n    \\\"google_news_3m\\\",\\n]\\ntps_word_embeddings_model_names = [\\n    \\\"enwiki\\\",\\n    \\\"semeval_2010_task_14\\\",\\n    \\\"cc.en.300.vec\\\",\\n    \\\"glove.840B.300d\\\",\\n    \\\"GoogleNews-vectors-negative300\\\",\\n]\\ntps_word_embeddings_is_external = [False, False, True, True, True]\\ntps_word_embeddings_paths = [\\n    join(word2vec_training_dir, \\\"word2vec_enwiki_jan_2021_word2phrase\\\"),\\n    join(word2vec_training_dir, \\\"word2vec_semeval_2010_task_14\\\"),\\n    join(word_embeddings_data_dir, \\\"fastText\\\"),\\n    join(word_embeddings_data_dir, \\\"GloVe\\\"),\\n    join(word_embeddings_data_dir, \\\"GoogleNews\\\"),\\n]\\ntps_vs_gs_key = \\\"TPS_n vs. GS\\\"\\ntps_vs_synsets_key = \\\"TPS_n vs. synsets\\\"\\ntps_vs_frequency_key = \\\"TPS_n vs. frequency\\\"\\nnum_top_k_words_frequencies = 10000\";\n",
       "                var nbb_formatted_code = \"# Constants\\ntps_neighbourhood_sizes = [10, 40, 50, 60, 100]\\ntps_word_embeddings_names = [\\n    \\\"enwiki\\\",\\n    \\\"semeval_2010_task_14\\\",\\n    \\\"fasttext_cc_300d\\\",\\n    \\\"glove_cc_840b_300d\\\",\\n    \\\"google_news_3m\\\",\\n]\\ntps_word_embeddings_model_names = [\\n    \\\"enwiki\\\",\\n    \\\"semeval_2010_task_14\\\",\\n    \\\"cc.en.300.vec\\\",\\n    \\\"glove.840B.300d\\\",\\n    \\\"GoogleNews-vectors-negative300\\\",\\n]\\ntps_word_embeddings_is_external = [False, False, True, True, True]\\ntps_word_embeddings_paths = [\\n    join(word2vec_training_dir, \\\"word2vec_enwiki_jan_2021_word2phrase\\\"),\\n    join(word2vec_training_dir, \\\"word2vec_semeval_2010_task_14\\\"),\\n    join(word_embeddings_data_dir, \\\"fastText\\\"),\\n    join(word_embeddings_data_dir, \\\"GloVe\\\"),\\n    join(word_embeddings_data_dir, \\\"GoogleNews\\\"),\\n]\\ntps_vs_gs_key = \\\"TPS_n vs. GS\\\"\\ntps_vs_synsets_key = \\\"TPS_n vs. synsets\\\"\\ntps_vs_frequency_key = \\\"TPS_n vs. frequency\\\"\\nnum_top_k_words_frequencies = 10000\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Constants\n",
    "tps_neighbourhood_sizes = [10, 40, 50, 60, 100]\n",
    "tps_word_embeddings_names = [\n",
    "    \"enwiki\",\n",
    "    \"semeval_2010_task_14\",\n",
    "    \"fasttext_cc_300d\",\n",
    "    \"glove_cc_840b_300d\",\n",
    "    \"google_news_3m\",\n",
    "]\n",
    "tps_word_embeddings_model_names = [\n",
    "    \"enwiki\",\n",
    "    \"semeval_2010_task_14\",\n",
    "    \"cc.en.300.vec\",\n",
    "    \"glove.840B.300d\",\n",
    "    \"GoogleNews-vectors-negative300\",\n",
    "]\n",
    "tps_word_embeddings_is_external = [False, False, True, True, True]\n",
    "tps_word_embeddings_paths = [\n",
    "    join(word2vec_training_dir, \"word2vec_enwiki_jan_2021_word2phrase\"),\n",
    "    join(word2vec_training_dir, \"word2vec_semeval_2010_task_14\"),\n",
    "    join(word_embeddings_data_dir, \"fastText\"),\n",
    "    join(word_embeddings_data_dir, \"GloVe\"),\n",
    "    join(word_embeddings_data_dir, \"GoogleNews\"),\n",
    "]\n",
    "tps_vs_gs_key = \"TPS_n vs. GS\"\n",
    "tps_vs_synsets_key = \"TPS_n vs. synsets\"\n",
    "tps_vs_frequency_key = \"TPS_n vs. frequency\"\n",
    "num_top_k_words_frequencies = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading enwiki word embeddings...\n",
      "Done!\n",
      "Find words in vocabulary that have synsets in Wordnet...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb39e54da504cf69d46ca5dab0f4e59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4430850 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading semeval_2010_task_14 word embeddings...\n",
      "Done!\n",
      "Find words in vocabulary that have synsets in Wordnet...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65de5cc8447f4581a52136f3e9a1446c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/121726 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fasttext_cc_300d word embeddings words...\n",
      "Find words in vocabulary that have synsets in Wordnet...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85918bdd149b4786bd1560cbee35a79c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading glove_cc_840b_300d word embeddings words...\n",
      "Find words in vocabulary that have synsets in Wordnet...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f2a09a25ca411787ab4a3589868d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2196017 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google_news_3m word embeddings words...\n",
      "Find words in vocabulary that have synsets in Wordnet...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ab7d86d2ec482cbbeb2528e80edd33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"# Create tables\\ntps_experiment_table_dicts = []\\ntps_experiment_table_col_sizes_dicts = []\\nfor (\\n    word_embeddings_name,\\n    word_embeddings_model_name,\\n    word_embeddings_path,\\n    word_embeddings_is_external,\\n) in zip(\\n    tps_word_embeddings_names,\\n    tps_word_embeddings_model_names,\\n    tps_word_embeddings_paths,\\n    tps_word_embeddings_is_external,\\n):\\n    # Load data\\n    if word_embeddings_is_external:\\n        print(f\\\"Loading {word_embeddings_name} word embeddings words...\\\")\\n        model_words_filepath = join(\\n            word_embeddings_path, f\\\"{word_embeddings_model_name}_words.txt\\\"\\n        )\\n        with open(model_words_filepath, \\\"r\\\") as words_file:\\n            model_words = np.array(words_file.read().split(\\\"\\\\n\\\"))\\n        word_to_int = {word: i for i, word in enumerate(model_words)}\\n    else:\\n        print(f\\\"Loading {word_embeddings_name} word embeddings...\\\")\\n        w2v_training_output = load_model_training_output(\\n            model_training_output_dir=word_embeddings_path,\\n            model_name=\\\"word2vec\\\",\\n            dataset_name=word_embeddings_model_name,\\n        )\\n        model_words = w2v_training_output[\\\"words\\\"]\\n        word_to_int = w2v_training_output[\\\"word_to_int\\\"]\\n        word_counts = w2v_training_output[\\\"word_counts\\\"]\\n        print(\\\"Done!\\\")\\n    col_sizes_result_dict = {}\\n    \\n    # Filter SemEval words in vocabulary\\n    semeval_target_words_in_vocab_filter = [\\n        i for i, word in enumerate(semeval_target_words) if word in word_to_int\\n    ]\\n    semeval_target_words_in_vocab = semeval_target_words[\\n        semeval_target_words_in_vocab_filter\\n    ]\\n    semeval_gs_clusters_in_vocab = semeval_gs_clusters[\\n        semeval_target_words_in_vocab_filter\\n    ]\\n    col_sizes_result_dict[tps_vs_gs_key] = len(semeval_gs_clusters_in_vocab)\\n\\n    # Find words in vocabulary that have synsets in Wordnet\\n    wordnet_synsets_words_in_vocab_meanings = []\\n    print(\\\"Find words in vocabulary that have synsets in Wordnet...\\\")\\n    for word in tqdm(model_words):\\n        num_synsets_word = len(wn.synsets(word))\\n        if num_synsets_word > 0:\\n            wordnet_synsets_words_in_vocab_meanings.append(num_synsets_word)\\n    col_sizes_result_dict[tps_vs_synsets_key] = len(wordnet_synsets_words_in_vocab_meanings)\\n\\n    result_dict: dict = {\\n        \\\"n\\\": tps_neighbourhood_sizes,\\n        tps_vs_gs_key: [],\\n        tps_vs_synsets_key: [],\\n    }\\n    if not word_embeddings_is_external:\\n        result_dict[tps_vs_frequency_key] = []\\n\\n    # Fill in dictionary\\n    for n_size in tps_neighbourhood_sizes:\\n\\n        # TPS_n score vs. GS\\n        tps_scores_semeval = np.load(\\n            join(\\n                topological_polysemy_experimentation_dir,\\n                word_embeddings_name,\\n                f\\\"tps_{n_size}_vs_gs.npy\\\",\\n            )\\n        )\\n        tps_score_vs_gs_correlation, _ = pearsonr(\\n            x=tps_scores_semeval, y=semeval_gs_clusters_in_vocab\\n        )\\n        result_dict[tps_vs_gs_key].append(tps_score_vs_gs_correlation)\\n\\n        # TPS_n score vs. Synsets\\n        tps_scores_wordnet_synsets = np.load(\\n            join(\\n                topological_polysemy_experimentation_dir,\\n                word_embeddings_name,\\n                f\\\"tps_{n_size}_vs_synsets.npy\\\",\\n            )\\n        )\\n        tps_score_vs_wordnet_synsets_correlation, _ = pearsonr(\\n            x=tps_scores_wordnet_synsets, y=wordnet_synsets_words_in_vocab_meanings\\n        )\\n        result_dict[tps_vs_synsets_key].append(tps_score_vs_wordnet_synsets_correlation)\\n\\n        # TPS_n score vs. frequency\\n        if not word_embeddings_is_external:\\n            tps_score_word_frequencies = np.load(\\n                join(\\n                    topological_polysemy_experimentation_dir,\\n                    word_embeddings_name,\\n                    f\\\"tps_{n_size}_vs_frequency.npy\\\",\\n                )\\n            )\\n            tps_score_vs_word_frequency_correlation, _ = pearsonr(\\n                x=tps_score_word_frequencies,\\n                y=word_counts[:num_top_k_words_frequencies],\\n            )\\n            result_dict[tps_vs_frequency_key].append(\\n                tps_score_vs_word_frequency_correlation\\n            )\\n            col_sizes_result_dict[tps_vs_frequency_key] = len(tps_score_word_frequencies)\\n    tps_experiment_table_dicts.append(result_dict)\\n    tps_experiment_table_col_sizes_dicts.append(col_sizes_result_dict)\\n\\n    # Free resources\\n    del wordnet_synsets_words_in_vocab_meanings\\n    if word_embeddings_is_external:\\n        del model_words\\n        del word_to_int\\n    else:\\n        del w2v_training_output\\n        del model_words\\n        del word_to_int\\n        del word_counts\";\n",
       "                var nbb_formatted_code = \"# Create tables\\ntps_experiment_table_dicts = []\\ntps_experiment_table_col_sizes_dicts = []\\nfor (\\n    word_embeddings_name,\\n    word_embeddings_model_name,\\n    word_embeddings_path,\\n    word_embeddings_is_external,\\n) in zip(\\n    tps_word_embeddings_names,\\n    tps_word_embeddings_model_names,\\n    tps_word_embeddings_paths,\\n    tps_word_embeddings_is_external,\\n):\\n    # Load data\\n    if word_embeddings_is_external:\\n        print(f\\\"Loading {word_embeddings_name} word embeddings words...\\\")\\n        model_words_filepath = join(\\n            word_embeddings_path, f\\\"{word_embeddings_model_name}_words.txt\\\"\\n        )\\n        with open(model_words_filepath, \\\"r\\\") as words_file:\\n            model_words = np.array(words_file.read().split(\\\"\\\\n\\\"))\\n        word_to_int = {word: i for i, word in enumerate(model_words)}\\n    else:\\n        print(f\\\"Loading {word_embeddings_name} word embeddings...\\\")\\n        w2v_training_output = load_model_training_output(\\n            model_training_output_dir=word_embeddings_path,\\n            model_name=\\\"word2vec\\\",\\n            dataset_name=word_embeddings_model_name,\\n        )\\n        model_words = w2v_training_output[\\\"words\\\"]\\n        word_to_int = w2v_training_output[\\\"word_to_int\\\"]\\n        word_counts = w2v_training_output[\\\"word_counts\\\"]\\n        print(\\\"Done!\\\")\\n    col_sizes_result_dict = {}\\n\\n    # Filter SemEval words in vocabulary\\n    semeval_target_words_in_vocab_filter = [\\n        i for i, word in enumerate(semeval_target_words) if word in word_to_int\\n    ]\\n    semeval_target_words_in_vocab = semeval_target_words[\\n        semeval_target_words_in_vocab_filter\\n    ]\\n    semeval_gs_clusters_in_vocab = semeval_gs_clusters[\\n        semeval_target_words_in_vocab_filter\\n    ]\\n    col_sizes_result_dict[tps_vs_gs_key] = len(semeval_gs_clusters_in_vocab)\\n\\n    # Find words in vocabulary that have synsets in Wordnet\\n    wordnet_synsets_words_in_vocab_meanings = []\\n    print(\\\"Find words in vocabulary that have synsets in Wordnet...\\\")\\n    for word in tqdm(model_words):\\n        num_synsets_word = len(wn.synsets(word))\\n        if num_synsets_word > 0:\\n            wordnet_synsets_words_in_vocab_meanings.append(num_synsets_word)\\n    col_sizes_result_dict[tps_vs_synsets_key] = len(\\n        wordnet_synsets_words_in_vocab_meanings\\n    )\\n\\n    result_dict: dict = {\\n        \\\"n\\\": tps_neighbourhood_sizes,\\n        tps_vs_gs_key: [],\\n        tps_vs_synsets_key: [],\\n    }\\n    if not word_embeddings_is_external:\\n        result_dict[tps_vs_frequency_key] = []\\n\\n    # Fill in dictionary\\n    for n_size in tps_neighbourhood_sizes:\\n\\n        # TPS_n score vs. GS\\n        tps_scores_semeval = np.load(\\n            join(\\n                topological_polysemy_experimentation_dir,\\n                word_embeddings_name,\\n                f\\\"tps_{n_size}_vs_gs.npy\\\",\\n            )\\n        )\\n        tps_score_vs_gs_correlation, _ = pearsonr(\\n            x=tps_scores_semeval, y=semeval_gs_clusters_in_vocab\\n        )\\n        result_dict[tps_vs_gs_key].append(tps_score_vs_gs_correlation)\\n\\n        # TPS_n score vs. Synsets\\n        tps_scores_wordnet_synsets = np.load(\\n            join(\\n                topological_polysemy_experimentation_dir,\\n                word_embeddings_name,\\n                f\\\"tps_{n_size}_vs_synsets.npy\\\",\\n            )\\n        )\\n        tps_score_vs_wordnet_synsets_correlation, _ = pearsonr(\\n            x=tps_scores_wordnet_synsets, y=wordnet_synsets_words_in_vocab_meanings\\n        )\\n        result_dict[tps_vs_synsets_key].append(tps_score_vs_wordnet_synsets_correlation)\\n\\n        # TPS_n score vs. frequency\\n        if not word_embeddings_is_external:\\n            tps_score_word_frequencies = np.load(\\n                join(\\n                    topological_polysemy_experimentation_dir,\\n                    word_embeddings_name,\\n                    f\\\"tps_{n_size}_vs_frequency.npy\\\",\\n                )\\n            )\\n            tps_score_vs_word_frequency_correlation, _ = pearsonr(\\n                x=tps_score_word_frequencies,\\n                y=word_counts[:num_top_k_words_frequencies],\\n            )\\n            result_dict[tps_vs_frequency_key].append(\\n                tps_score_vs_word_frequency_correlation\\n            )\\n            col_sizes_result_dict[tps_vs_frequency_key] = len(\\n                tps_score_word_frequencies\\n            )\\n    tps_experiment_table_dicts.append(result_dict)\\n    tps_experiment_table_col_sizes_dicts.append(col_sizes_result_dict)\\n\\n    # Free resources\\n    del wordnet_synsets_words_in_vocab_meanings\\n    if word_embeddings_is_external:\\n        del model_words\\n        del word_to_int\\n    else:\\n        del w2v_training_output\\n        del model_words\\n        del word_to_int\\n        del word_counts\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create tables\n",
    "tps_experiment_table_dicts = []\n",
    "tps_experiment_table_col_sizes_dicts = []\n",
    "for (\n",
    "    word_embeddings_name,\n",
    "    word_embeddings_model_name,\n",
    "    word_embeddings_path,\n",
    "    word_embeddings_is_external,\n",
    ") in zip(\n",
    "    tps_word_embeddings_names,\n",
    "    tps_word_embeddings_model_names,\n",
    "    tps_word_embeddings_paths,\n",
    "    tps_word_embeddings_is_external,\n",
    "):\n",
    "    # Load data\n",
    "    if word_embeddings_is_external:\n",
    "        print(f\"Loading {word_embeddings_name} word embeddings words...\")\n",
    "        model_words_filepath = join(\n",
    "            word_embeddings_path, f\"{word_embeddings_model_name}_words.txt\"\n",
    "        )\n",
    "        with open(model_words_filepath, \"r\") as words_file:\n",
    "            model_words = np.array(words_file.read().split(\"\\n\"))\n",
    "        word_to_int = {word: i for i, word in enumerate(model_words)}\n",
    "    else:\n",
    "        print(f\"Loading {word_embeddings_name} word embeddings...\")\n",
    "        w2v_training_output = load_model_training_output(\n",
    "            model_training_output_dir=word_embeddings_path,\n",
    "            model_name=\"word2vec\",\n",
    "            dataset_name=word_embeddings_model_name,\n",
    "        )\n",
    "        model_words = w2v_training_output[\"words\"]\n",
    "        word_to_int = w2v_training_output[\"word_to_int\"]\n",
    "        word_counts = w2v_training_output[\"word_counts\"]\n",
    "        print(\"Done!\")\n",
    "    col_sizes_result_dict = {}\n",
    "\n",
    "    # Filter SemEval words in vocabulary\n",
    "    semeval_target_words_in_vocab_filter = [\n",
    "        i for i, word in enumerate(semeval_target_words) if word in word_to_int\n",
    "    ]\n",
    "    semeval_target_words_in_vocab = semeval_target_words[\n",
    "        semeval_target_words_in_vocab_filter\n",
    "    ]\n",
    "    semeval_gs_clusters_in_vocab = semeval_gs_clusters[\n",
    "        semeval_target_words_in_vocab_filter\n",
    "    ]\n",
    "    col_sizes_result_dict[tps_vs_gs_key] = len(semeval_gs_clusters_in_vocab)\n",
    "\n",
    "    # Find words in vocabulary that have synsets in Wordnet\n",
    "    wordnet_synsets_words_in_vocab_meanings = []\n",
    "    print(\"Find words in vocabulary that have synsets in Wordnet...\")\n",
    "    for word in tqdm(model_words):\n",
    "        num_synsets_word = len(wn.synsets(word))\n",
    "        if num_synsets_word > 0:\n",
    "            wordnet_synsets_words_in_vocab_meanings.append(num_synsets_word)\n",
    "    col_sizes_result_dict[tps_vs_synsets_key] = len(\n",
    "        wordnet_synsets_words_in_vocab_meanings\n",
    "    )\n",
    "\n",
    "    result_dict: dict = {\n",
    "        \"n\": tps_neighbourhood_sizes,\n",
    "        tps_vs_gs_key: [],\n",
    "        tps_vs_synsets_key: [],\n",
    "    }\n",
    "    if not word_embeddings_is_external:\n",
    "        result_dict[tps_vs_frequency_key] = []\n",
    "\n",
    "    # Fill in dictionary\n",
    "    for n_size in tps_neighbourhood_sizes:\n",
    "\n",
    "        # TPS_n score vs. GS\n",
    "        tps_scores_semeval = np.load(\n",
    "            join(\n",
    "                topological_polysemy_experimentation_dir,\n",
    "                word_embeddings_name,\n",
    "                f\"tps_{n_size}_vs_gs.npy\",\n",
    "            )\n",
    "        )\n",
    "        tps_score_vs_gs_correlation, _ = pearsonr(\n",
    "            x=tps_scores_semeval, y=semeval_gs_clusters_in_vocab\n",
    "        )\n",
    "        result_dict[tps_vs_gs_key].append(tps_score_vs_gs_correlation)\n",
    "\n",
    "        # TPS_n score vs. Synsets\n",
    "        tps_scores_wordnet_synsets = np.load(\n",
    "            join(\n",
    "                topological_polysemy_experimentation_dir,\n",
    "                word_embeddings_name,\n",
    "                f\"tps_{n_size}_vs_synsets.npy\",\n",
    "            )\n",
    "        )\n",
    "        tps_score_vs_wordnet_synsets_correlation, _ = pearsonr(\n",
    "            x=tps_scores_wordnet_synsets, y=wordnet_synsets_words_in_vocab_meanings\n",
    "        )\n",
    "        result_dict[tps_vs_synsets_key].append(tps_score_vs_wordnet_synsets_correlation)\n",
    "\n",
    "        # TPS_n score vs. frequency\n",
    "        if not word_embeddings_is_external:\n",
    "            tps_score_word_frequencies = np.load(\n",
    "                join(\n",
    "                    topological_polysemy_experimentation_dir,\n",
    "                    word_embeddings_name,\n",
    "                    f\"tps_{n_size}_vs_frequency.npy\",\n",
    "                )\n",
    "            )\n",
    "            tps_score_vs_word_frequency_correlation, _ = pearsonr(\n",
    "                x=tps_score_word_frequencies,\n",
    "                y=word_counts[:num_top_k_words_frequencies],\n",
    "            )\n",
    "            result_dict[tps_vs_frequency_key].append(\n",
    "                tps_score_vs_word_frequency_correlation\n",
    "            )\n",
    "            col_sizes_result_dict[tps_vs_frequency_key] = len(\n",
    "                tps_score_word_frequencies\n",
    "            )\n",
    "    tps_experiment_table_dicts.append(result_dict)\n",
    "    tps_experiment_table_col_sizes_dicts.append(col_sizes_result_dict)\n",
    "\n",
    "    # Free resources\n",
    "    del wordnet_synsets_words_in_vocab_meanings\n",
    "    if word_embeddings_is_external:\n",
    "        del model_words\n",
    "        del word_to_int\n",
    "    else:\n",
    "        del w2v_training_output\n",
    "        del model_words\n",
    "        del word_to_int\n",
    "        del word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- enwiki --\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TPS_n vs. GS</th>\n",
       "      <th>TPS_n vs. synsets</th>\n",
       "      <th>TPS_n vs. frequency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.353</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.181</td>\n",
       "      <td>-0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>-0.380</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>-0.381</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>-0.380</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>-0.033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TPS_n vs. GS': 98, 'TPS_n vs. synsets': 144412, 'TPS_n vs. frequency': 10000}\n",
      "-- semeval_2010_task_14 --\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TPS_n vs. GS</th>\n",
       "      <th>TPS_n vs. synsets</th>\n",
       "      <th>TPS_n vs. frequency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.300</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>0.102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.201</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>-0.194</td>\n",
       "      <td>-0.304</td>\n",
       "      <td>0.116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>-0.169</td>\n",
       "      <td>-0.306</td>\n",
       "      <td>0.110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.310</td>\n",
       "      <td>0.098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TPS_n vs. GS': 100, 'TPS_n vs. synsets': 62111, 'TPS_n vs. frequency': 10000}\n",
      "-- fasttext_cc_300d --\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TPS_n vs. GS</th>\n",
       "      <th>TPS_n vs. synsets</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.240</td>\n",
       "      <td>0.114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.289</td>\n",
       "      <td>0.110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>-0.199</td>\n",
       "      <td>0.108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TPS_n vs. GS': 100, 'TPS_n vs. synsets': 230175}\n",
      "-- glove_cc_840b_300d --\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TPS_n vs. GS</th>\n",
       "      <th>TPS_n vs. synsets</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.103</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TPS_n vs. GS': 100, 'TPS_n vs. synsets': 249352}\n",
      "-- google_news_3m --\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TPS_n vs. GS</th>\n",
       "      <th>TPS_n vs. synsets</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.446</td>\n",
       "      <td>-0.095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.446</td>\n",
       "      <td>-0.166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>-0.436</td>\n",
       "      <td>-0.174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>-0.417</td>\n",
       "      <td>-0.193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TPS_n vs. GS': 100, 'TPS_n vs. synsets': 207119}\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"# Visualize tables\\npd.set_option(\\\"display.precision\\\", 3)\\nfor name, table_dict, col_sizes_dict in zip(tps_word_embeddings_names, tps_experiment_table_dicts, tps_experiment_table_col_sizes_dicts):\\n    print(f\\\"-- {name} --\\\")\\n    table_df = pd.DataFrame(table_dict)\\n    table_df.set_index(\\\"n\\\", inplace=True)\\n    display(HTML(table_df.to_html()))\\n    print(col_sizes_dict)\";\n",
       "                var nbb_formatted_code = \"# Visualize tables\\npd.set_option(\\\"display.precision\\\", 3)\\nfor name, table_dict, col_sizes_dict in zip(\\n    tps_word_embeddings_names,\\n    tps_experiment_table_dicts,\\n    tps_experiment_table_col_sizes_dicts,\\n):\\n    print(f\\\"-- {name} --\\\")\\n    table_df = pd.DataFrame(table_dict)\\n    table_df.set_index(\\\"n\\\", inplace=True)\\n    display(HTML(table_df.to_html()))\\n    print(col_sizes_dict)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize tables\n",
    "pd.set_option(\"display.precision\", 3)\n",
    "for name, table_dict, col_sizes_dict in zip(\n",
    "    tps_word_embeddings_names,\n",
    "    tps_experiment_table_dicts,\n",
    "    tps_experiment_table_col_sizes_dicts,\n",
    "):\n",
    "    print(f\"-- {name} --\")\n",
    "    table_df = pd.DataFrame(table_dict)\n",
    "    table_df.set_index(\"n\", inplace=True)\n",
    "    display(HTML(table_df.to_html()))\n",
    "    print(col_sizes_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
