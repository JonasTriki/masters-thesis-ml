{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-water",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-paris",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "rng_seed = 399\n",
    "np.random.seed(rng_seed)\n",
    "import persim\n",
    "import joblib\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.preprocessing import minmax_scale, RobustScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "try:\n",
    "    # Disable all GPUS\n",
    "    tf.config.set_visible_devices([], \"GPU\")\n",
    "    visible_devices = tf.config.get_visible_devices()\n",
    "    for device in visible_devices:\n",
    "        assert device.device_type != \"GPU\"\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass\n",
    "\n",
    "from tensorflow.keras.activations import relu\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# Directory constants\n",
    "root_code_dir = \"..\"\n",
    "output_dir = join(root_code_dir, \"output\")\n",
    "word2vec_training_dir = join(output_dir, \"word2vec_training\")\n",
    "word2vec_ann_indices_dir = join(output_dir, \"word2vec_ann_indices\")\n",
    "word2vec_cluster_analysis_dir = join(output_dir, \"word2vec_cluster_analysis\")\n",
    "\n",
    "# Extend sys path for importing custom Python files\n",
    "import sys\n",
    "\n",
    "sys.path.append(root_code_dir)\n",
    "\n",
    "from topological_data_analysis.topological_polysemy import tps\n",
    "from word_embeddings.word2vec import load_model_training_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-outside",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-specialist",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_meaning_train_data = pd.read_csv(\"data/word_meaning_train_data.csv\")\n",
    "word_meaning_test_data = pd.read_csv(\"data/word_meaning_test_data.csv\")\n",
    "word_meaning_data_cols = word_meaning_train_data.columns.values\n",
    "word_meaning_data_feature_cols = np.array(\n",
    "    [col for col in word_meaning_data_cols if col.startswith(\"X_\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-description",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train\")\n",
    "word_meaning_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-intake",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test\")\n",
    "word_meaning_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-colombia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into X and y\n",
    "X_train = minmax_scale(word_meaning_train_data[word_meaning_data_feature_cols].values)\n",
    "X_test = minmax_scale(word_meaning_test_data[word_meaning_data_feature_cols].values)\n",
    "y_train = word_meaning_train_data[\"y\"].values\n",
    "y_test = word_meaning_test_data[\"y\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-christopher",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load output from training word2vec\n",
    "w2v_training_output = load_model_training_output(\n",
    "    model_training_output_dir=join(\n",
    "        word2vec_training_dir, \"word2vec_enwiki_jan_2021_word2phrase\"\n",
    "    ),\n",
    "    model_name=\"word2vec\",\n",
    "    dataset_name=\"enwiki\",\n",
    "    return_normalized_embeddings=True,\n",
    ")\n",
    "last_embedding_weights_normalized = w2v_training_output[\n",
    "    \"last_embedding_weights_normalized\"\n",
    "]\n",
    "words = w2v_training_output[\"words\"]\n",
    "word_to_int = w2v_training_output[\"word_to_int\"]\n",
    "word_counts = w2v_training_output[\"word_counts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-record",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SemEval-2010 task 14 words\n",
    "semeval_2010_14_word_senses = joblib.load(\n",
    "    join(\n",
    "        \"..\", \"topological_data_analysis\", \"data\", \"semeval_2010_14_word_senses.joblib\"\n",
    "    )\n",
    ")\n",
    "semeval_target_words = np.array(list(semeval_2010_14_word_senses[\"all\"].keys()))\n",
    "semeval_target_words_in_vocab_filter = [\n",
    "    i for i, word in enumerate(semeval_target_words) if word in word_to_int\n",
    "]\n",
    "semeval_target_words_in_vocab = semeval_target_words[\n",
    "    semeval_target_words_in_vocab_filter\n",
    "]\n",
    "semeval_gs_clusters = np.array(list(semeval_2010_14_word_senses[\"all\"].values()))\n",
    "semeval_gs_clusters_in_vocab = semeval_gs_clusters[semeval_target_words_in_vocab_filter]\n",
    "\n",
    "num_semeval_words = len(semeval_target_words_in_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-papua",
   "metadata": {},
   "source": [
    "## Do modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-algorithm",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred_vs_true_labels(\n",
    "    pred_labels: np.ndarray, true_labels: np.ndarray, xlabel: str, ylabel: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    TODO: Docs\n",
    "    \"\"\"\n",
    "    pred_true_corr, _ = pearsonr(pred_labels, true_labels)\n",
    "    pred_true_mse = mean_squared_error(true_labels, pred_labels)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.scatter(pred_labels, true_labels)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(f\"Pred/True: correlation: {pred_true_corr:.3f}, MSE: {pred_true_mse:.3f}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-personality",
   "metadata": {},
   "source": [
    "### Hyperparameter search using grid search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-southwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "cv = 5\n",
    "max_iter = 100000\n",
    "lasso_alphas = np.linspace(0.0001, 0.9999, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-phase",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso regression\n",
    "lasso_cv = LassoCV(\n",
    "    alphas=lasso_alphas,\n",
    "    random_state=rng_seed,\n",
    "    max_iter=max_iter,\n",
    "    n_jobs=-1,\n",
    "    cv=cv,\n",
    ")\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "print(f\"Selected alpha: {lasso_cv.alpha_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-thanksgiving",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sorted_feature_weights_indices = np.argsort(np.abs(lasso_cv.coef_))[::-1]\n",
    "list(\n",
    "    zip(\n",
    "        word_meaning_data_feature_cols[sorted_feature_weights_indices],\n",
    "        lasso_cv.coef_[sorted_feature_weights_indices],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-sampling",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = lasso_cv.predict(X_train)\n",
    "plot_pred_vs_true_labels(\n",
    "    y_pred, y_train, xlabel=\"Pred number of synsets\", ylabel=\"Synsets in Wordnet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-situation",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = lasso_cv.predict(X_test)\n",
    "plot_pred_vs_true_labels(\n",
    "    y_pred_test, y_test, xlabel=\"Pred number of meanings\", ylabel=\"Clusters in GS\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
