{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-water",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-paris",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "rng_seed = 399\n",
    "np.random.seed(rng_seed)\n",
    "import persim\n",
    "import joblib\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "try:\n",
    "    # Disable all GPUS\n",
    "    tf.config.set_visible_devices([], \"GPU\")\n",
    "    visible_devices = tf.config.get_visible_devices()\n",
    "    for device in visible_devices:\n",
    "        assert device.device_type != \"GPU\"\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass\n",
    "\n",
    "from tensorflow.keras.activations import relu\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# Directory constants\n",
    "root_code_dir = \"..\"\n",
    "output_dir = join(root_code_dir, \"output\")\n",
    "word2vec_training_dir = join(output_dir, \"word2vec_training\")\n",
    "word2vec_ann_indices_dir = join(output_dir, \"word2vec_ann_indices\")\n",
    "word2vec_cluster_analysis_dir = join(output_dir, \"word2vec_cluster_analysis\")\n",
    "\n",
    "# Extend sys path for importing custom Python files\n",
    "import sys\n",
    "\n",
    "sys.path.append(root_code_dir)\n",
    "\n",
    "from topological_data_analysis.topological_polysemy import tps\n",
    "from word_embeddings.word2vec import load_model_training_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-outside",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-specialist",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_meaning_train_data = pd.read_csv(\"data/word_meaning_train_data.csv\")\n",
    "word_meaning_test_data = pd.read_csv(\"data/word_meaning_test_data.csv\")\n",
    "word_meaning_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-colombia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into X and y\n",
    "tps_neighbourhood_sizes = np.linspace(start=10, stop=100, num=10, dtype=int)\n",
    "feature_cols = np.array(\n",
    "    [\n",
    "        \"estimated_id\",\n",
    "        *[f\"tps_{n_size}\" for n_size in tps_neighbourhood_sizes],\n",
    "        *[f\"tps_{n_size}_bottle\" for n_size in tps_neighbourhood_sizes],\n",
    "    ]\n",
    ")\n",
    "X_train = word_meaning_train_data[feature_cols].values\n",
    "X_test = word_meaning_test_data[feature_cols].values\n",
    "y_train = word_meaning_train_data[\"y\"].values\n",
    "y_test = word_meaning_test_data[\"y\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-christopher",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load output from training word2vec\n",
    "w2v_training_output = load_model_training_output(\n",
    "    model_training_output_dir=join(\n",
    "        word2vec_training_dir, \"word2vec_enwiki_jan_2021_word2phrase\"\n",
    "    ),\n",
    "    model_name=\"word2vec\",\n",
    "    dataset_name=\"enwiki\",\n",
    "    return_normalized_embeddings=True,\n",
    ")\n",
    "last_embedding_weights_normalized = w2v_training_output[\n",
    "    \"last_embedding_weights_normalized\"\n",
    "]\n",
    "words = w2v_training_output[\"words\"]\n",
    "word_to_int = w2v_training_output[\"word_to_int\"]\n",
    "word_counts = w2v_training_output[\"word_counts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-offense",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SemEval-2010 task 14 words\n",
    "semeval_2010_14_word_senses = joblib.load(\n",
    "    join(\n",
    "        \"..\", \"topological_data_analysis\", \"data\", \"semeval_2010_14_word_senses.joblib\"\n",
    "    )\n",
    ")\n",
    "semeval_target_words = np.array(list(semeval_2010_14_word_senses[\"all\"].keys()))\n",
    "semeval_target_words_in_vocab_filter = [\n",
    "    i for i, word in enumerate(semeval_target_words) if word in word_to_int\n",
    "]\n",
    "semeval_target_words_in_vocab = semeval_target_words[\n",
    "    semeval_target_words_in_vocab_filter\n",
    "]\n",
    "semeval_gs_clusters = np.array(list(semeval_2010_14_word_senses[\"all\"].values()))\n",
    "semeval_gs_clusters_in_vocab = semeval_gs_clusters[semeval_target_words_in_vocab_filter]\n",
    "\n",
    "num_semeval_words = len(semeval_target_words_in_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-papua",
   "metadata": {},
   "source": [
    "## Do modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-issue",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(\n",
    "    input_dim: int,\n",
    "    lasso_alpha: float,\n",
    "    learning_rate: float,\n",
    ") -> Model:\n",
    "    \"\"\"\n",
    "    TODO: Docs\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(input_dim,), name=\"input\")\n",
    "\n",
    "    # Output layer\n",
    "    output_layer = Dense(1, kernel_regularizer=l1(lasso_alpha), name=\"output\")(\n",
    "        input_layer\n",
    "    )\n",
    "\n",
    "    # Create model and compile\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer=SGD(learning_rate=learning_rate), loss=MSE)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-algorithm",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr(true_labels: np.ndarray, pred_labels: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    TODO: Docs\n",
    "    \"\"\"\n",
    "    corr, _ = pearsonr(pred_labels, true_labels)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(pred_labels, true_labels)\n",
    "    plt.title(f\"true/pred correlation: {corr:.3f}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-evaluation",
   "metadata": {},
   "source": [
    "### Hyperparameter search using grid search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-kansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasRegressor(build_fn=create_model)\n",
    "lasso_alphas = np.linspace(0.0001, 0.9999, 1000)\n",
    "param_grid = {\n",
    "    \"input_dim\": [X_train.shape[1]],\n",
    "    \"lasso_alpha\": lasso_alphas,\n",
    "    \"learning_rate\": [0.025, 0.001, 0.0025, 0.0001, 0.00025],\n",
    "}\n",
    "n_epochs = 200\n",
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-cleaning",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv=5)\n",
    "grid_result = grid.fit(X_train, y_train, epochs=n_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-demand",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(wm_model_hist.history[\"loss\"])\n",
    "plt.title(\"model loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-phase",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso regression\n",
    "lasso_cv = LassoCV(\n",
    "    alphas=lasso_alphas, random_state=rng_seed, max_iter=100000, n_jobs=-1, cv=5\n",
    ")\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lasso_cv.predict(X_train)\n",
    "mean_squared_error(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-hostel",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_cv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-thanksgiving",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sorted_coeffs = np.argsort((lasso_cv.coef_))[::-1]\n",
    "list(zip(feature_cols[sorted_coeffs], lasso_cv.coef_[sorted_coeffs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-sampling",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-budget",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = lasso_cv.predict(X_test)\n",
    "mean_squared_error(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-cover",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(x=y_pred_test, y=y_test)\n",
    "plt.title(f\"Correlation: {pearsonr(y_pred_test, y_test)[0]:.5f}\")\n",
    "plt.xlabel(\"Pred number of meanings\")\n",
    "plt.ylabel(\"GS\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
