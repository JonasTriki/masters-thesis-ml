{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from os import makedirs\n",
    "from os.path import join\n",
    "import pickle\n",
    "import numpy as np\n",
    "rng_seed = 399\n",
    "np.random.seed(rng_seed)\n",
    "from scipy.spatial.distance import pdist, cdist, squareform\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "from tqdm.auto import tqdm\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from hdbscan import HDBSCAN\n",
    "from DBCV import DBCV\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from scipy.cluster.hierarchy import dendrogram, fcluster\n",
    "\n",
    "from umap import UMAP\n",
    "import plotly.offline as pyo\n",
    "pyo.init_notebook_mode()\n",
    "\n",
    "from importlib import reload\n",
    "import utils\n",
    "reload(utils)\n",
    "\n",
    "from utils import get_model_checkpoint_filepaths, pairwise_cosine_distances, cosine_distance\n",
    "from analysis_utils import create_linkage_matrix, words_in_clusters, plot_silhouette_scores\n",
    "from word_embeddings.eval_utils import plot_word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get last word embeddings from training\n",
    "checkpoint_filepaths_dict = get_model_checkpoint_filepaths(\n",
    "    output_dir=\"../output/word2vec_training/word2vec_enwiki_sept_2020_word2phrase\",\n",
    "    model_name=\"word2vec\",\n",
    "    dataset_name=\"enwiki\",\n",
    ")\n",
    "last_embedding_weights_filepath = checkpoint_filepaths_dict[\"intermediate_embedding_weight_filepaths\"][-1]\n",
    "last_embedding_weights = np.load(last_embedding_weights_filepath, mmap_mode=\"r\").astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load words and create word to int lookup dict\n",
    "with open(checkpoint_filepaths_dict[\"train_words_filepath\"], \"r\") as file:\n",
    "    words = np.array(file.read().split(\"\\n\"))\n",
    "word_to_int = {word: i for i, word in enumerate(words)}\n",
    "vocab_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Precompute cosine distance matrix\n",
    "word_embeddings_to_precompute = last_embedding_weights[:vocab_size]\n",
    "word_embeddings_distances = pairwise_cosine_distances(word_embeddings_to_precompute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform clustering with HDBSCAN\n",
    "# TODO: Split into function\n",
    "# TODO: Add saving result to disk\n",
    "hdbscan_param_grid = ParameterGrid({\n",
    "    \"min_cluster_size\": [2, 4, 8, 16, 32, 64, 128, 256], \n",
    "    \"min_samples\": [1, 2, 4, 8, 16, 32, 64, 128, 256],\n",
    "})\n",
    "hdbscan_cluster_labels = []\n",
    "hdbscan_dbcv_scores = []\n",
    "for param_grid in tqdm(hdbscan_param_grid, desc=\"Performing HDBSCAN clustering\"):\n",
    "    hdbscan_clustering = HDBSCAN(\n",
    "        **param_grid,\n",
    "        metric=\"cosine\",\n",
    "        algorithm=\"generic\",\n",
    "        core_dist_n_jobs=-1,\n",
    "        gen_min_span_tree=True,\n",
    "    )\n",
    "    cluster_labels_pred = hdbscan_clustering.fit_predict(word_embeddings_to_precompute)\n",
    "    hdbscan_cluster_labels.append(cluster_labels_pred)\n",
    "    \n",
    "    dbcv_score = hdbscan_clustering.relative_validity_\n",
    "    hdbscan_dbcv_scores.append(dbcv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create DBCV score plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "should_pred_cluster_labels = False\n",
    "ks = [2, 3, 4, 5, 10, 50, 100, 150, 200, 300, 400, 500, 750, 1000, 1500, 2000, 3000, 4000, 5000, 6000, 7000, 8000]\n",
    "if should_pred_cluster_labels:\n",
    "    pred_cluster_labels = agglomerative_cluster_number_search(\n",
    "        cluster_numbers=ks,\n",
    "        clusterings=agglomerative_clusterings,\n",
    "        linkages=list(agglomerative_clusterings.keys()),\n",
    "        word_embeddings_distances=word_embeddings_distances,\n",
    "        output_filepath_suffix=\"agglomerative_labels\",\n",
    "        output_dir=\"../output/word2vec_cluster_analysis\",\n",
    "        model_name=\"word2vec\",\n",
    "        dataset_name=\"enwiki\"\n",
    "    )\n",
    "else:\n",
    "    with open(\"../output/word2vec_cluster_analysis/word2vec-enwiki-agglomerative_labels.pkl\", \"rb\") as file:\n",
    "        pred_cluster_labels = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute cluster size ratios (maximum cluster size / minimum cluster size)\n",
    "most_common_cluster_sizes = {}\n",
    "for linkage, labels in best_cluster_labels.items():\n",
    "    print(f\"Linkage: {linkage}\")\n",
    "    labels_unique, labels_counts = np.unique(labels, return_counts=True)\n",
    "    num_clusters = len(labels_unique)\n",
    "    max_cluster_size = max(labels_counts)\n",
    "    min_cluster_size = min(labels_counts)\n",
    "    cluster_size_ratio = max_cluster_size / min_cluster_size\n",
    "    print(f\"{num_clusters} clusters: max={max_cluster_size}, min={min_cluster_size}, ratio={cluster_size_ratio}\")\n",
    "    \n",
    "    # Plot distribution of cluster sizes\n",
    "    hist_plot = sns.histplot(labels_counts, bins=max_cluster_size)\n",
    "    bar_heights = [h.get_height() for h in hist_plot.patches]\n",
    "    most_common_cluster_sizes[linkage] = np.arange(1, max_cluster_size + 1)[np.argsort(bar_heights)[::-1]]\n",
    "    plt.show()\n",
    "\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the words corresponding to the different clusters (biggest, smallest, etc.)\n",
    "cluster_words, cluster_sizes = words_in_clusters(\n",
    "    cluster_labels=best_cluster_labels[\"complete\"],\n",
    "    words=words[:vocab_size]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only inspect clusters with at least 5 words in them\n",
    "min_cluster_size = 5\n",
    "filter_min_cluster_size_mask = cluster_sizes >= min_cluster_size\n",
    "cluster_sizes_filtered = cluster_sizes[filter_min_cluster_size_mask]\n",
    "cluster_words_filtered = cluster_words[filter_min_cluster_size_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sorted_cluster_indices = np.argsort(cluster_sizes_filtered)[::-1]\n",
    "\n",
    "num_clusters_print = 10\n",
    "print(f\"-- {num_clusters_print} largest clusters --\")\n",
    "for i in range(num_clusters_print):\n",
    "    print(cluster_words_filtered[sorted_cluster_indices[i]])\n",
    "    \n",
    "print(f\"-- {num_clusters_print} smallest clusters --\")\n",
    "for i in range(1, num_clusters_print + 1):\n",
    "    print(cluster_words_filtered[sorted_cluster_indices[-i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect words from clusters whose cluster numbers is the most common\n",
    "for cluster_words in cluster_words[cluster_sizes == most_common_cluster_sizes[\"complete\"][0]][:25]:\n",
    "    print(cluster_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_embeddings_transformed = UMAP(n_components=2, metric=\"precomputed\").fit_transform(word_embeddings_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing first 1000 words using UMAP (TODO: remove this?)\n",
    "plot_word_vectors(\n",
    "    words_to_plot=words[:1000],\n",
    "    transformed_word_embeddings=word_embeddings_transformed[:1000],\n",
    "    word_to_int=word_to_int,\n",
    "    word_colors=best_cluster_labels[\"complete\"][:1000]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
