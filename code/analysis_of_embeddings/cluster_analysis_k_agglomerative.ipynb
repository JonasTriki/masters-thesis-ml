{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from os import makedirs\n",
    "from os.path import join\n",
    "import pickle\n",
    "import numpy as np\n",
    "rng_seed = 399\n",
    "np.random.seed(rng_seed)\n",
    "from scipy.spatial.distance import pdist, cdist, squareform\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "from tqdm.auto import tqdm\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from scipy.cluster.hierarchy import dendrogram, fcluster\n",
    "\n",
    "from umap import UMAP\n",
    "import plotly.offline as pyo\n",
    "pyo.init_notebook_mode()\n",
    "\n",
    "from utils import get_model_checkpoint_filepaths, pairwise_cosine_distances\n",
    "from analysis_utils import create_linkage_matrix, words_in_clusters, plot_silhouette_scores\n",
    "from word_embeddings.eval_utils import plot_word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get last word embeddings from training\n",
    "checkpoint_filepaths_dict = get_model_checkpoint_filepaths(\n",
    "    output_dir=\"../output/word2vec_training/03-Nov-2020_11-01-00\",\n",
    "    model_name=\"word2vec\",\n",
    "    dataset_name=\"enwiki\",\n",
    ")\n",
    "last_embedding_weights_filepath = checkpoint_filepaths_dict[\"intermediate_embedding_weight_filepaths\"][-1]\n",
    "last_embedding_weights = np.load(last_embedding_weights_filepath, mmap_mode=\"r\").astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load words and create word to int lookup dict\n",
    "with open(checkpoint_filepaths_dict[\"train_words_filepath\"], \"r\") as file:\n",
    "    words = np.array(file.read().split(\"\\n\"))\n",
    "word_to_int = {word: i for i, word in enumerate(words)}\n",
    "vocab_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Precompute cosine distance matrix\n",
    "word_embeddings_to_precompute = last_embedding_weights[:vocab_size]\n",
    "word_embeddings_distances = pairwise_cosine_distances(word_embeddings_to_precompute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform agglomerative clustering\n",
    "agglomerative_clusterings = {}\n",
    "for linkage in [\"complete\", \"average\", \"single\"]:\n",
    "\n",
    "    # Fit clustering and create linkage matrix\n",
    "    agglomerative_clustering = AgglomerativeClustering(\n",
    "        n_clusters=None,\n",
    "        affinity=\"precomputed\",\n",
    "        linkage=linkage,\n",
    "        distance_threshold=0\n",
    "    )\n",
    "    agglomerative_clustering.fit(word_embeddings_distances)\n",
    "    \n",
    "    # Create required linkage matrix for fcluster function\n",
    "    agglomerative_clustering_linkage_matrix = create_linkage_matrix(agglomerative_clustering)\n",
    "    \n",
    "    # Set result in dict\n",
    "    agglomerative_clusterings[linkage] = {\n",
    "        \"clustering\": agglomerative_clustering,\n",
    "        \"linkage_matrix\": agglomerative_clustering_linkage_matrix\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agglomerative_cluster_number_search(\n",
    "    cluster_numbers: list,\n",
    "    clusterings: list,\n",
    "    linkages: list,\n",
    "    word_embeddings_distances: np.ndarray,\n",
    "    output_filepath_suffix: str,\n",
    "    output_dir: str = None,\n",
    "    model_name: str = None,\n",
    "    dataset_name: str = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    TODO: Docs\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Fit and predict cluster labels\n",
    "    cluster_labels = {}\n",
    "    print(f\"-- Fitting and predicting cluster labels for agglomerative clustering --\")\n",
    "    for linkage in linkages:\n",
    "        print(f\"Linkage: {linkage}\")\n",
    "        cluster_labels[linkage] = {\n",
    "            \"labels\": [],\n",
    "            \"metric_values\": [],\n",
    "            \"best_labels_idx\": -1\n",
    "        }\n",
    "        for k in tqdm(cluster_numbers):\n",
    "\n",
    "            linkage_matrix = clusterings[linkage][\"linkage_matrix\"]\n",
    "            cluster_labels_pred = fcluster(Z=linkage_matrix, criterion=\"maxclust\", t=k)\n",
    "            cluster_labels[linkage][\"labels\"].append(cluster_labels_pred)\n",
    "\n",
    "            cluster_metric_value = silhouette_score(word_embeddings_distances, cluster_labels_pred, metric=\"precomputed\")\n",
    "            cluster_labels[linkage][\"metric_values\"].append(cluster_metric_value)\n",
    "\n",
    "        cluster_labels[linkage][\"best_labels_idx\"] = np.argmax(cluster_labels[linkage][\"metric_values\"])\n",
    "\n",
    "    # Save to output dir\n",
    "    if output_dir is not None and model_name is not None and dataset_name is not None:\n",
    "        output_path = join(output_dir, f\"{model_name}-{dataset_name}-{output_filepath_suffix}.pkl\")\n",
    "        with open(output_path, \"wb\") as file:\n",
    "            pickle.dump(cluster_labels, file)\n",
    "    \n",
    "    return cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "should_pred_cluster_labels = True\n",
    "ks = [2, 3, 4, 5, 10, 50, 100, 150, 200, 300, 400, 500, 750, 1000, 1500, 2000, 3000, 4000, 5000, 6000, 7000, 8000]\n",
    "if should_pred_cluster_labels:\n",
    "    pred_cluster_labels = agglomerative_cluster_number_search(\n",
    "        cluster_numbers=ks,\n",
    "        clusterings=agglomerative_clusterings,\n",
    "        linkages=list(agglomerative_clusterings.keys()),\n",
    "        word_embeddings_distances=word_embeddings_distances,\n",
    "        output_filepath_suffix=\"agglomerative_labels\",\n",
    "        output_dir=\"../output/word2vec_cluster_analysis\",\n",
    "        model_name=\"word2vec\",\n",
    "        dataset_name=\"enwiki\"\n",
    "    )\n",
    "else:\n",
    "    with open(\"../output/word2vec_cluster_analysis/word2vec-enwiki-agglomerative_labels.pkl\", \"rb\") as file:\n",
    "        pred_cluster_labels = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for linkage in agglomerative_clusterings.keys():\n",
    "    print(f\"Linkage: {linkage}\")\n",
    "    plot_silhouette_scores(ks, pred_cluster_labels[linkage][\"metric_values\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom in at 3000-6000 clusters.\n",
    "should_pred_cluster_labels_zoomed = True\n",
    "ks_zoomed = np.linspace(3000, 6000, num=100, dtype=int)\n",
    "if should_pred_cluster_labels_zoomed:\n",
    "    pred_cluster_labels_zoomed = agglomerative_cluster_number_search(\n",
    "        cluster_numbers=ks_zoomed,\n",
    "        clusterings=agglomerative_clusterings,\n",
    "        linkages=[\"complete\", \"average\"],\n",
    "        word_embeddings_distances=word_embeddings_distances,\n",
    "        output_filepath_suffix=\"agglomerative_labels_zoomed\",\n",
    "        output_dir=\"../output/word2vec_cluster_analysis\",\n",
    "        model_name=\"word2vec\",\n",
    "        dataset_name=\"enwiki\"\n",
    "    )\n",
    "else:\n",
    "    with open(\"../output/word2vec_cluster_analysis/word2vec-enwiki-agglomerative_labels_zoomed.pkl\", \"rb\") as file:\n",
    "        pred_cluster_labels_zoomed = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cluster_labels = {}\n",
    "for linkage in [\"complete\", \"average\"]:\n",
    "    print(f\"Linkage: {linkage}\")\n",
    "    silhouette_scores = pred_cluster_labels_zoomed[linkage][\"metric_values\"]\n",
    "    best_labels_idx = pred_cluster_labels_zoomed[linkage][\"best_labels_idx\"]\n",
    "    \n",
    "    best_num_clusters = ks_zoomed[best_labels_idx]\n",
    "    print(f\"Best number of clusters: {best_num_clusters}\")\n",
    "    \n",
    "    best_cluster_labels[linkage] = pred_cluster_labels_zoomed[linkage][\"labels\"][best_labels_idx]\n",
    "    plot_silhouette_scores(ks_zoomed, silhouette_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute cluster size ratios (maximum cluster size / minimum cluster size)\n",
    "for linkage, labels in best_cluster_labels.items():\n",
    "    print(f\"Linkage: {linkage}\")\n",
    "    labels_unique, labels_counts = np.unique(labels, return_counts=True)\n",
    "    num_clusters = len(labels_unique)\n",
    "    max_cluster_size = max(labels_counts)\n",
    "    min_cluster_size = min(labels_counts)\n",
    "    cluster_size_ratio = max_cluster_size / min_cluster_size\n",
    "    print(f\"{num_clusters} clusters: max={max_cluster_size}, min={min_cluster_size}, ratio={cluster_size_ratio}\")\n",
    "    \n",
    "    # Plot distribution of cluster sizes\n",
    "    sns.histplot(labels_counts, bins=max_cluster_size)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the words corresponding to the different clusters (biggest, smallest, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_words, cluster_sizes = words_in_clusters(\n",
    "    cluster_labels=best_cluster_labels[\"complete\"],\n",
    "    words=words[:vocab_size]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biggest_cluster_idx = np.argmax(cluster_sizes)\n",
    "smallest_cluster_idx = np.argmin(cluster_sizes[cluster_sizes >= 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cluster_words[biggest_cluster_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Hva skjer med de andre tallene? Er de i clustre \"ved siden av\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_words[cluster_sizes >= 5][smallest_cluster_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Se på histogram og inspiser clustre (nummer) som forekommer ofte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing first 100 words using UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings_to_precompute.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings_transformed = UMAP(n_components=2, metric=\"precomputed\").fit_transform(word_embeddings_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_vectors(\n",
    "    words_to_plot=words[:1000],\n",
    "    transformed_word_embeddings=word_embeddings_transformed,\n",
    "    word_to_int=word_to_int,\n",
    "    word_colors=best_cluster_labels[\"complete\"][:1000]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - UMAP av kun tall?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
