{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, cdist, squareform\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from sklearn.cluster import KMeans, SpectralClustering, AgglomerativeClustering\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "\n",
    "from utils import get_model_checkpoint_filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get last word embeddings from training\n",
    "checkpoint_filepaths_dict = get_model_checkpoint_filepaths(\n",
    "    output_dir=\"../output/word2vec_training/03-Oct-2020_15-00-00\",\n",
    "    model_name=\"word2vec_sgns\",\n",
    "    dataset_name=\"enwiki\",\n",
    ")\n",
    "last_embedding_weights_filepath = checkpoint_filepaths_dict[\"intermediate_embedding_weight_filepaths\"][-1]\n",
    "last_embedding_weights = np.load(last_embedding_weights_filepath, mmap_mode=\"r\").astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dunn_index(X, labels):\n",
    "    \"\"\"\n",
    "    TODO: Docs\n",
    "    \"\"\"\n",
    "    unique_labels = np.unique(labels)\n",
    "    # print([len(pdist(X[labels==lab])) for lab in unique_labels])\n",
    "    diam = np.max([np.max(pdist(X[labels==lab]), initial=0) for lab in unique_labels])\n",
    "    sep = np.min([np.min(cdist(X[labels==unique_labels[i]], X[labels==unique_labels[j]])) \n",
    "                  for i in range(len(unique_labels)) for j in range(i)])\n",
    "    return sep/diam\n",
    "\n",
    "def sd_validity_index(X, labels):\n",
    "    \"\"\"\n",
    "    TODO: Docs\n",
    "    \"\"\"\n",
    "    unique_labels = np.unique(labels)\n",
    "    scat = np.mean([np.linalg.norm(np.var(X[labels==lab], axis=0)) \n",
    "                    for lab in unique_labels]) / np.linalg.norm(np.var(X, axis=0))\n",
    "    \n",
    "    centers = np.array([np.mean(X[labels==lab], axis=0) for lab in unique_labels])\n",
    "    center_dists = pdist(centers)\n",
    "    dis = np.sum(1/np.sum(squareform(center_dists), axis = 0)) * np.max(center_dists) / np.min(center_dists)\n",
    "    return scat + dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cluster_methods(\n",
    "    word_embeddings: np.ndarray,\n",
    "    vocab_size: int,\n",
    "    cluster_classes: list,\n",
    "    cluster_metrics: list,\n",
    "    cluster_numbers: list,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    TODO: Docs\n",
    "    \"\"\"\n",
    "    X = word_embeddings[:vocab_size]\n",
    "    clusterer_results = {clusterer_name: {} for clusterer_name, _, _ in cluster_classes}\n",
    "    for cluster_name, cluster_cls, kwargs in cluster_classes:\n",
    "        print(f\"--- Evaluating {cluster_name}... ---\")\n",
    "        if cluster_name != \"HDBSCAN\":\n",
    "            clusterer_results[cluster_name] = {\n",
    "                \"labels\": {},\n",
    "                \"metric_scores\": {metric_name: [] for metric_name, _, _ in cluster_metrics},\n",
    "                \"optimal_metric_score_indices\": {}\n",
    "            }\n",
    "            for k in tqdm(ks, desc=\"Fitting and predicting\"):\n",
    "                cls = cluster_cls(n_clusters=k, **kwargs)\n",
    "                cluster_labels = cls.fit_predict(X)\n",
    "                clusterer_results[cluster_name][\"labels\"][k] = cluster_labels\n",
    "                \n",
    "                for metric_name, metric_func, _ in cluster_metrics:\n",
    "                    metric_score = metric_func(X, cluster_labels)\n",
    "                    clusterer_results[cluster_name][\"metric_scores\"][metric_name].append(metric_score)\n",
    "                \n",
    "                for metric_name, _, metric_opt_value_idx_func in cluster_metrics:\n",
    "                    opt_value_idx = metric_opt_value_idx_func(\n",
    "                        clusterer_results[cluster_name][\"metric_scores\"][metric_name]\n",
    "                    )\n",
    "                    clusterer_results[cluster_name][\"optimal_metric_score_indices\"][metric_name] = opt_value_idx\n",
    "        else:\n",
    "            cls = cluster_cls()\n",
    "            cluster_labels = cls.fit_predict(X)\n",
    "            clusterer_results[cluster_name] = {\n",
    "                \"labels\": cluster_labels,\n",
    "                \"metric_scores\": {metric_name: [] for metric_name, _, _ in cluster_metrics},\n",
    "                \"optimal_metric_score_indices\": {}\n",
    "            }\n",
    "            \n",
    "            for metric_name, metric_func, _ in cluster_metrics:\n",
    "                metric_score = metric_func(X, cluster_labels)\n",
    "                print(f\"- {metric_name}: {metric_score:.3f}\")\n",
    "                clusterer_results[cluster_name][\"metric_scores\"][metric_name].append(metric_score)\n",
    "            \n",
    "            for metric_name, _, metric_opt_value_idx_func in cluster_metrics:\n",
    "                opt_value_idx = metric_opt_value_idx_func(\n",
    "                    clusterer_results[cluster_name][\"metric_scores\"][metric_name]\n",
    "                )\n",
    "                clusterer_results[cluster_name][\"optimal_metric_score_indices\"][metric_name] = opt_value_idx\n",
    "    return clusterer_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_classes = [\n",
    "    (\"K-means clustering\", KMeans, {}),\n",
    "    (\"Spectral clustering\", SpectralClustering, {\"n_jobs\": -1}),\n",
    "    (\"Agglomerative clustering\", AgglomerativeClustering, {}),\n",
    "    (\"HDBSCAN\", HDBSCAN, {\"core_dist_n_jobs\": -1})\n",
    "]\n",
    "cluster_metrics = [\n",
    "    (\"Average silhouette score\", silhouette_score, np.argmax),\n",
    "    (\"Davies-Bouldin score\", davies_bouldin_score, np.argmin),\n",
    "    (\"Dunn index\", dunn_index, np.argmax),\n",
    "    (\"SD validity index\", sd_validity_index, np.argmin)\n",
    "]\n",
    "max_cluster_num = 100\n",
    "ks = list(range(2, max_cluster_num + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform evaluation\n",
    "cluster_results_dict = evaluate_cluster_methods(\n",
    "    word_embeddings=last_embedding_weights,\n",
    "    vocab_size=1000,\n",
    "    cluster_classes=cluster_classes,\n",
    "    cluster_metrics=cluster_metrics,\n",
    "    cluster_numbers=ks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_cluster_results(cluster_results: dict) -> None:\n",
    "    \"\"\"\n",
    "    TODO: Docs\n",
    "    \"\"\"\n",
    "    for cluster_name, cluster_content in cluster_results_dict.items():\n",
    "        metric_names = list(cluster_content[\"metric_scores\"].keys())\n",
    "        \n",
    "        if cluster_name != \"HDBSCAN\":\n",
    "            print(f\"-- Visualizing metrics for {cluster_name} --\")\n",
    "            ks = list(cluster_content[\"labels\"].keys())\n",
    "            fig, axes = plt.subplots(nrows=1, ncols=len(metric_names), figsize=(3.25 * len(metric_names), 3))\n",
    "            for metric_name, ax in zip(metric_names, axes.ravel()):\n",
    "                metric_scores = cluster_content[\"metric_scores\"][metric_name]\n",
    "                optimal_metric_score_idx = cluster_content[\"optimal_metric_score_indices\"][metric_name]\n",
    "                ax.set_title(metric_name)\n",
    "                ax.set_xlabel(\"Cluster number\")\n",
    "                ax.set_ylabel(\"Metric value\")\n",
    "                ax.scatter(ks, metric_scores)\n",
    "                ax.plot(ks, metric_scores)\n",
    "                ax.plot(ks[optimal_metric_score_idx], metric_scores[optimal_metric_score_idx], 'ro')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"-- Printing metrics for {cluster_name} --\")\n",
    "            for metric_name in metric_names:\n",
    "                metric_value = cluster_content[\"metric_scores\"][metric_name][0]\n",
    "                print(f\"{metric_name}: {metric_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_cluster_results(cluster_results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Synonym words should be in the same cluster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
