\pagenumbering{roman}

\begin{abstract}
\noindent
Over the last few years, advances in natural language processing have enabled us to learn more from textual data. To this end, word embedding models (e.g. word2vec) learn vectorized representations of words by training on big sets of texts (e.g. the entire Wikipedia corpus). Word embeddings are well-researched in terms of their syntactic and semantic relations, but little has been done in the field of clustering and topological data analysis (TDA). To expand on these fields, we perform cluster analysis of word embeddings and use recent methods from TDA to identify polysemous words. Our results show that we are effectively able to cluster word embeddings into groups of varying sizes. Results also revealed that it was more difficult than anticipated to identify polysemous words, and our proposed supervised models attempt to overcome this. Finally, this thesis emphasizes the usefulness of TDA of word embeddings.
\end{abstract}

\renewcommand{\abstractname}{Acknowledgements}
\begin{abstract}
I would first and foremost like to thank my supervisor, Nello Blaser, for his outstanding guidance during the work of this thesis. I would also like to thank my fellow graduate students, particularly Naphat, for the long lunch breaks and the helpful academic discussions. Finally, I would like to thank my friends and family for all their love and support.

\vspace{1cm}
\hspace*{\fill}\texttt{Jonas Folkvord Triki}\\ 
\hspace*{\fill} 01 June, 2021
\end{abstract}
\newpage