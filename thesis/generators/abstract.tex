\pagenumbering{roman}

\begin{abstract}
\noindent
Over the last few years, advances in natural language processing has enabled us to learn more from textual data. To this end, word embedding models (e.g. word2vec) learn vectorized representations of words by training on big sets of texts (e.g. the entire Wikipedia corpus). Word embeddings are well-researched in terms of its syntactic and semantic relations, but little has been done in the field of clustering and topological data analysis (TDA). To expand on these fields, we perform cluster analysis of word embeddings and use recent methods from TDA in order to to identify polysemous words. Our results show that we are effectively able to cluster word embeddings into groups of varying sizes. Results also revealed that it was more difficult than anticipated to identify polysemous words, and our proposed supervised models attempt to overcome this. Finally, this thesis emphasizes the usefulness of TDA of word embeddings.
\end{abstract}

\renewcommand{\abstractname}{Acknowledgements}
\begin{abstract}
Est suavitate gubergren referrentur an, ex mea dolor eloquentiam, novum ludus suscipit in nec. Ea mea essent prompta constituam, has ut novum prodesset vulputate. Ad noster electram pri, nec sint accusamus dissentias at. Est ad laoreet fierent invidunt, ut per assueverit conclusionemque. An electram efficiendi mea.

\vspace{1cm}
\hspace*{\fill}\texttt{Jonas Folkvord Triki}\\ 
\hspace*{\fill} 01 June, 2021
\end{abstract}
\newpage