\chapter{Analysis of Word Embeddings}
\label{chap:analysis-of-word-embeddings}
In this chapter, we will use methods from machine learning to analyze word embeddings. Due to the scope of the thesis, we will mainly analyze word embeddings from the word2vec (\cref{sec:word2vec}) model using Skip-gram and negative sampling. We will also run some of the analysis methods on published word embeddings from external papers, in particular in \cref{sec:polysemous-words-prediction}.

We will first describe how we trained and evaluated our word2vec implementation. In particular, we will explain the data preprocessing steps, the implementation specifics and hyperparameter choices. We will also show how we evaluated our trained word2vec model. Second, we will perform cluster analysis on word embeddings to look for deeper structure. In particular, we will compare clustering algorithms trained on word embeddings, using internal cluster validation methods, and investigate clustering of distinct groups of words. Third, we will investigate two methods from topological data analysis on word embeddings. Lastly, we end the chapter by creating two supervised models for estimating the number of word meanings, using results from topological data analysis and intrinsic dimension estimation. The supervised models are trained and evaluation results are visualized.

To perform the analyses in this chapter, we utilized the Python programming language with some key Python packages: \path{numpy} \cite{2020NumPy-Array} (efficient vector- and matrix manipulation), \path{scikit-learn} \cite{ScikitLearn2011} and \path{scipy} \cite{2020SciPy-NMeth} (general methods from machine learning), \path{matplotlib} \cite{Matplotlib2007} and \path{seaborn} \cite{seaborn2021} (tools for data visualization), \path{joblib} \cite{joblib2021} (data dumping to file), \path{sharedmem} \cite{sharedmem2020} (parallelization of trivial jobs) and \path{fastdist} \cite{fastdist2021} (fast distance calculations in Python). The analysis code was ran on a machine with two GPUs (GeForce RTX 2080 Ti $\times2$), one CPU (Intel i9-7900X @ 3.30GHz) and 64 GB of RAM. The computer was running an Ubuntu 18.04.5 operating system. In practice, we were only allotted to use a subset of the resources, as it was a shared computer by the research group in machine learning at the University of Bergen. Finally, the code used to perform the analyses are publicly available and can be accessed through GitHub in \cite{Triki2021}.

% Include sections
\input{chapters/analysis_of_word_embeddings/training_and_evaluation_of_word2vec}
\input{chapters/analysis_of_word_embeddings/word_clustering}
\input{chapters/analysis_of_word_embeddings/polysemous_words_prediction}