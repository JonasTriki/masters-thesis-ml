\chapter{Analysis of Word Embeddings}
\label{chap:analysis-of-word-embeddings}
In this chapter, we will use methods from machine learning to analyze word embeddings. Due to the scope of the thesis, we will mainly analyze word embeddings from the word2vec model (\cref{sec:word2vec}) using Skip-gram and negative sampling. We will also run some of the analysis methods on published word embeddings from external papers, in particular in \cref{sec:polysemous-words-prediction}.

Firstly, we will describe how we trained and evaluated our word2vec implementation. In particular, we will explain the data preprocessing steps, the implementation specifics and hyperparameter choices. We will also show how we evaluated our trained word2vec model. Secondly, we will perform cluster analysis on word embeddings to look for deeper structure. In particular, we will compare clustering algorithms trained on word embeddings, using internal cluster validation methods, and investigate the clustering of distinct groups of words. Thirdly, we will look at the application of two methods from topological data analysis on word embeddings. We end the chapter by creating two supervised models for estimating the number of word meanings, using the results from topological data analysis and intrinsic dimension estimation. We train the supervised models and visualize their evaluation results

To perform the analyses in this chapter, we utilized the Python programming language with some key Python packages: \path{numpy} \cite{2020NumPy-Array} (efficient vector and matrix manipulation), \path{scikit-learn} \cite{ScikitLearn2011} and \path{scipy} \cite{2020SciPy-NMeth} (general methods from machine learning), \path{matplotlib} \cite{Matplotlib2007} and \path{seaborn} \cite{seaborn2021} (tools for data visualization), \path{joblib} \cite{joblib2021} (data dumping to file), \path{sharedmem} \cite{sharedmem2020} (parallelization of trivial jobs) and \path{fastdist} \cite{fastdist2021} (fast distance calculations in Python). We ran the analysis code on a machine with two GPUs (GeForce RTX 2080 Ti $\times2$), one CPU (Intel i9-7900X @ 3.30GHz) and 64 GB of RAM. The computer was running an Ubuntu 18.04.5 operating system. In practice, we were only allotted to use a subset of the resources, as it was a shared computer by the research group in machine learning at the University of Bergen. Finally, the code used to perform the analyses is publicly available via GitHub in \cite{Triki2021}.

% Include sections
\input{chapters/analysis_of_word_embeddings/training_and_evaluation_of_word2vec}
\input{chapters/analysis_of_word_embeddings/word_clustering}
\input{chapters/analysis_of_word_embeddings/polysemous_words_prediction}