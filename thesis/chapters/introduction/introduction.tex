\chapter{Introduction}
\label{chap:introduction}
Natural language processing (NLP) is a field of study which focuses on the interactions between computers and the natural human language. In modern NLP applications, it is common to see the usage of vector embedding algorithms, such as the word2vec techniques introduced in \cite{mikolov2013a}. Word2vec learns vectorized representations of words (called word embeddings) by training on big text sets (e.g. whole Wikipedia). Word embeddings are created such that the semantics of a word is reflected by its word embedding. For example, the word \textit{solution} is similar to the word \textit{chemistry}, but is also related to words such as \textit{answer} or \textit{equation}. By inspecting the word embeddings of such words, we find that they are highly similar.

Classically, word embeddings are analyzed using analogy and cluster analysis. In this thesis, we will first perform a similar analogy and cluster analysis, by using general methods from machine learning, e.g. clustering algorithms and dimensionality reduction methods, to find a richer description of word embeddings. In particular, we will first explain how we trained and evaluated our word2vec model, before proceeding onto clustering of word embeddings using clustering algorithms. We will validate the results from the cluster analysis using internal cluster validation methods and visualize our results using dimensionality reduction methods.

Furthermore, we also see that the word \textit{solution} can have multiple meanings, depending on the context it is used in. We call such words \textit{polysemous}, and the task of determining polysemous words in NLP is a difficult problem. To measure and determine polysemous words from word embeddings, the measure of topological polysemy was introduced in \cite{jakubowski2020topology}. The topological polysemy method seeks to identify singular word embeddings, which the authors claim reflect polysemy. After performing the cluster analysis on word embeddings, we will in this thesis investigate the method of topological polysemy by applying it to various word embedding models. Our goal is to recreate the results shown in \cite{jakubowski2020topology} for a general word embedding model, which further strengthens their proposed method and results. We will also look at another algorithm called Geometric Anomaly detection (GAD). GAD seeks to identify singular points in the data, similar to what the topological polysemy method does. Following, we will compare the results using topological polysemy and GAD to show their relation. Next, we look at the application of intrinsic dimension estimation (ID) methods on word embeddings and show the relationship between the estimated local ID of a word to the number of word meanings. We would like to, in particular, see if the estimated local ID of word embeddings can help us to predict the number of word meanings. Finally, we propose two supervised methods for predicting polysemous words, using results from topological polysemy, GAD and ID estimation methods.

This thesis is structured as follows. In \cref{chap:background} we give the technical and theoretical background required for this thesis. Following, in \cref{chap:analysis-of-word-embeddings} we perform our analysis of word embeddings, explaining training and evaluation steps for our word2vec model, perform clustering of word embeddings, and finally, analyze methods for polysemous words prediction. In \cref{chap:summary-and-conclusion}, we summarize and conclude the thesis. Lastly, in \cref{chap:future-work} we look at ideas for future work related to the thesis.