\chapter{Introduction}
\label{chap:introduction}
Natural language processing (NLP) is a field of study which focuses on the interactions between computers and the natural human language. In modern NLP applications, it is common to see the usage of vector embedding algorithms, such as the word2vec techniques introduced in \cite{mikolov2013a}. Word2vec learns vectorized representations of words (called word embeddings) by training on big sets of text (e.g. whole Wikipedia). Word embeddings are created such that the semantics of a word is reflected by its word embedding. For example, the word "solution" is similar to the word "chemistry", but is also related to words such as "answer" or "equation". By inspecting the word embeddings of such words, we find that they are highly similar.

Classically, word embeddings are analyzed using cluster and sentiment analysis. In this thesis, however, we will use general methods from machine learning (e.g. clustering algorithms and dimensionality reduction methods), as well as methods topological data analysis in order to find a richer description of word embeddings. In particular, we first explain how we trained and evaluated our word2vec model, before proceeding onto clustering of word embeddings using clustering algorithms. We validate the results from the cluster analysis using internal cluster validation methods and visualize our results using dimensionality reduction methods. Furthermore, we use methods from topological data analysis and intrinsic dimension estimation for predicting the number of word meanings. In particular, we investigate the notion of topological polysemy, which seeks to identify singular word embeddings, reflecting polysemy. Additionally, we used the Geometric Anomaly Detection algorithms on word embeddings and compared results from topological polysemy, as it also seeks to identify singular points in the data. Finally, we propose two supervised methods for prediction of polysemous words.

This thesis is structured as follows. In \cref{chap:background} we give the technical and theoretical background required for this thesis. Following, in \cref{chap:analysis-of-word-embeddings} we perform our analysis of word embeddings, explaining training and evaluation steps for our word2vec model, perform clustering of word embeddings, and finally, analyze methods for polysemous words prediction. In \cref{chap:conclusion-and-future-work}, we conclude the thesis and look at ideas for future work.